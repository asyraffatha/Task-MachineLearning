{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW0PvpATqOxHWjJt/q6JMq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyraffatha/Task-MachineLearning/blob/main/Week%2014/RNN_dan_Deep_RNN_Model_Asyraff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Mpj9oHOqRUWY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VavKDCEXRVni",
        "outputId": "06bd11c8-d584-41e9-d56d-27e6a2446b96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset in your Google Drive\n",
        "file_path = \"/content/drive/MyDrive/Week 14/data.csv\"  # Ganti dengan lokasi file Anda"
      ],
      "metadata": {
        "id": "QnSS_0qkRfJY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "data = pd.read_csv(file_path, delimiter=';')\n",
        "data.columns = data.columns.str.strip()"
      ],
      "metadata": {
        "id": "A3-7yKgxR8Xh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing steps (from the original code)\n",
        "data.columns = data.columns.str.strip()\n",
        "data = data.select_dtypes(include=[np.number]).dropna()\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "X = data_scaled[:, :-1]\n",
        "y = data_scaled[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OanwjMDER8sw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "nuR_3I1dSFPP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, pooling=\"max\"):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.pooling = pooling\n",
        "        if pooling == \"max\":\n",
        "            self.pool = lambda x: torch.max(x, dim=1)[0]\n",
        "        else:\n",
        "            self.pool = lambda x: torch.mean(x, dim=1)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn_out, _ = self.rnn(x)\n",
        "        pooled_out = self.pool(rnn_out)\n",
        "        output = self.fc(pooled_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "m5HFpJQ3SIY_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment parameters\n",
        "hidden_sizes = [16, 32, 64]\n",
        "pooling_types = [\"max\", \"avg\"]\n",
        "epochs_list = [5, 50, 100, 250, 350]\n",
        "optimizers = [\"SGD\", \"RMSProp\", \"Adam\"]\n",
        "input_size = X_train.shape[1]\n",
        "output_size = 1  # Regression output\n",
        "\n",
        "results = []"
      ],
      "metadata": {
        "id": "tftdWbLsSOzw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiments\n",
        "for hidden_size in hidden_sizes:\n",
        "    for pooling in pooling_types:\n",
        "        for optimizer_name in optimizers:\n",
        "            for epochs in epochs_list:\n",
        "                model = RNNModel(input_size, hidden_size, output_size, pooling=pooling)\n",
        "                if optimizer_name == \"SGD\":\n",
        "                    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "                elif optimizer_name == \"RMSProp\":\n",
        "                    optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "                elif optimizer_name == \"Adam\":\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "                criterion = nn.MSELoss()\n",
        "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "                # Training loop\n",
        "                for epoch in range(epochs):\n",
        "                    model.train()\n",
        "                    for batch_X, batch_y in train_loader:\n",
        "                        optimizer.zero_grad()\n",
        "                        outputs = model(batch_X.unsqueeze(1))  # Add sequence dimension\n",
        "                        loss = criterion(outputs.squeeze(), batch_y)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    scheduler.step()\n",
        "\n",
        "                # Evaluation\n",
        "                model.eval()\n",
        "                y_pred = []\n",
        "                with torch.no_grad():\n",
        "                    for batch_X, _ in test_loader:\n",
        "                        outputs = model(batch_X.unsqueeze(1))\n",
        "                        y_pred.extend(outputs.squeeze().tolist())\n",
        "\n",
        "                # Store results\n",
        "                results.append({\n",
        "                    \"hidden_size\": hidden_size,\n",
        "                    \"pooling\": pooling,\n",
        "                    \"optimizer\": optimizer_name,\n",
        "                    \"epochs\": epochs,\n",
        "                    \"mse_loss\": criterion(torch.tensor(y_pred), y_test_tensor).item()\n",
        "                })"
      ],
      "metadata": {
        "id": "wR4aG6ycSQp3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to DataFrame for analysis\n",
        "results_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "_gV_3zuiSUDf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to CSV\n",
        "results_df.to_csv(\"experiment_results.csv\", index=False)\n",
        "print(\"Experiments completed. Results saved to 'experiment_results.csv'.\")"
      ],
      "metadata": {
        "id": "WiNZhoFMSvkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a3de61-8574-4e1e-ed49-ba6584dd6561"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiments completed. Results saved to 'experiment_results.csv'.\n"
          ]
        }
      ]
    }
  ]
}