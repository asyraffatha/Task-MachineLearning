{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmSWmyLzr8Pk84g2qHle/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyraffatha/Task-MachineLearning/blob/main/Week%2010/MLP_Regression_Asyraff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ktkz5kCLWrnR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20fa571-a013-4345-cb5c-df1193113f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17eiIZLcaNnm",
        "outputId": "b7124c2c-96a1-4b1a-f007-7a4576197f94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/StudentsPerformance.csv'\n"
      ],
      "metadata": {
        "id": "jjq9kfttag51"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset from Google Drive\n",
        "data = pd.read_csv('/content/drive/MyDrive/StudentsPerformance.csv')\n",
        "\n",
        "# Display dataset information\n",
        "print(\"Shape of the dataset:\", data.shape)\n",
        "print(\"Columns:\", data.columns)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZE0L1O6a0_Y",
        "outputId": "0fdf43f7-b7a2-4215-8e04-0efad7b6387c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dataset: (1000, 8)\n",
            "Columns: Index(['gender', 'race/ethnicity', 'parental level of education', 'lunch',\n",
            "       'test preparation course', 'math score', 'reading score',\n",
            "       'writing score'],\n",
            "      dtype='object')\n",
            "   gender race/ethnicity parental level of education         lunch  \\\n",
            "0  female        group B           bachelor's degree      standard   \n",
            "1  female        group C                some college      standard   \n",
            "2  female        group B             master's degree      standard   \n",
            "3    male        group A          associate's degree  free/reduced   \n",
            "4    male        group C                some college      standard   \n",
            "\n",
            "  test preparation course  math score  reading score  writing score  \n",
            "0                    none          72             72             74  \n",
            "1               completed          69             90             88  \n",
            "2                    none          90             95             93  \n",
            "3                    none          47             57             44  \n",
            "4                    none          76             78             75  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbkZOizqbI_q",
        "outputId": "0f9e8e41-a3c6-42c6-b902-cbed0e78644c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/StudentsPerformance.csv')\n",
        "\n",
        "# Target variable\n",
        "target = \"math score\"\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[target])\n",
        "y = data[target]\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
        "numerical_columns = X.select_dtypes(include=[\"number\"]).columns\n",
        "\n",
        "# Encode categorical columns and scale numerical columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numerical_columns),\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_columns),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transformations\n",
        "X = preprocessor.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert target to NumPy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "CQbSaYBJbKQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72df4248-f2bf-443c-f88b-7717f81dfb9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (800, 14)\n",
            "Shape of y_train: (800,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Prepare DataLoader\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define MLP Model\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation):\n",
        "        super(MLPModel, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        for hidden_size in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(activation())\n",
        "            prev_size = hidden_size\n",
        "        layers.append(nn.Linear(prev_size, 1))  # Output layer\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Example: MLP with 1 hidden layer of 32 neurons and ReLU\n",
        "input_size = X_train.shape[1]\n",
        "hidden_layers = [32]\n",
        "activation = nn.ReLU\n",
        "\n",
        "model = MLPModel(input_size, hidden_layers, activation)\n"
      ],
      "metadata": {
        "id": "yxM9OfFHbYC0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, epochs, learning_rate):\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch).squeeze()\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, epochs=10, learning_rate=0.01)\n"
      ],
      "metadata": {
        "id": "L99DBUlkbbsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4963b903-6b13-409e-cac4-46396fa50f33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 4192.9948828125\n",
            "Epoch 2/10, Loss: 2254.4563134765626\n",
            "Epoch 3/10, Loss: 359.83982238769534\n",
            "Epoch 4/10, Loss: 143.04339294433595\n",
            "Epoch 5/10, Loss: 120.70472564697266\n",
            "Epoch 6/10, Loss: 110.18603637695313\n",
            "Epoch 7/10, Loss: 102.13147003173827\n",
            "Epoch 8/10, Loss: 93.40913421630859\n",
            "Epoch 9/10, Loss: 84.79495895385742\n",
            "Epoch 10/10, Loss: 76.61862106323242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter combinations\n",
        "hidden_layers_list = [[32], [32, 16], [64, 32, 16]]  # 1, 2, 3 hidden layers\n",
        "activation_functions = [nn.ReLU, nn.Sigmoid]\n",
        "epochs_list = [10, 50]\n",
        "learning_rates = [0.01, 0.001]\n",
        "batch_sizes = [32, 128]\n",
        "\n",
        "# Loop through parameters\n",
        "results = []\n",
        "for hidden_layers in hidden_layers_list:\n",
        "    for activation in activation_functions:\n",
        "        for epochs in epochs_list:\n",
        "            for lr in learning_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    print(f\"Testing: {hidden_layers}, {activation.__name__}, Epochs={epochs}, LR={lr}, Batch={batch_size}\")\n",
        "\n",
        "                    # Update DataLoader with new batch size\n",
        "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "                    # Initialize model\n",
        "                    model = MLPModel(input_size, hidden_layers, activation)\n",
        "\n",
        "                    # Train model\n",
        "                    train_model(model, train_loader, epochs, lr)\n"
      ],
      "metadata": {
        "id": "NVMVXyJDbeij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02116dc6-00b4-4f68-e732-9de1b9cd6ad0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: [32], ReLU, Epochs=10, LR=0.01, Batch=32\n",
            "Epoch 1/10, Loss: 4373.2120703125\n",
            "Epoch 2/10, Loss: 2703.905654296875\n",
            "Epoch 3/10, Loss: 503.6021936035156\n",
            "Epoch 4/10, Loss: 149.6594497680664\n",
            "Epoch 5/10, Loss: 124.94898376464843\n",
            "Epoch 6/10, Loss: 115.94307983398437\n",
            "Epoch 7/10, Loss: 106.61046630859374\n",
            "Epoch 8/10, Loss: 97.39236297607422\n",
            "Epoch 9/10, Loss: 88.69726654052734\n",
            "Epoch 10/10, Loss: 80.80475082397462\n",
            "Testing: [32], ReLU, Epochs=10, LR=0.01, Batch=128\n",
            "Epoch 1/10, Loss: 4561.210797991072\n",
            "Epoch 2/10, Loss: 4371.184500558035\n",
            "Epoch 3/10, Loss: 4067.1446358816966\n",
            "Epoch 4/10, Loss: 3586.7580915178573\n",
            "Epoch 5/10, Loss: 3006.585693359375\n",
            "Epoch 6/10, Loss: 2282.3587472098216\n",
            "Epoch 7/10, Loss: 1508.4278215680804\n",
            "Epoch 8/10, Loss: 822.2852434430804\n",
            "Epoch 9/10, Loss: 365.98080008370533\n",
            "Epoch 10/10, Loss: 170.4018293108259\n",
            "Testing: [32], ReLU, Epochs=10, LR=0.001, Batch=32\n",
            "Epoch 1/10, Loss: 4613.676787109375\n",
            "Epoch 2/10, Loss: 4561.2983203125\n",
            "Epoch 3/10, Loss: 4491.4065234375\n",
            "Epoch 4/10, Loss: 4393.6442578125\n",
            "Epoch 5/10, Loss: 4257.780302734375\n",
            "Epoch 6/10, Loss: 4080.239755859375\n",
            "Epoch 7/10, Loss: 3860.769443359375\n",
            "Epoch 8/10, Loss: 3606.19041015625\n",
            "Epoch 9/10, Loss: 3317.347109375\n",
            "Epoch 10/10, Loss: 3007.235068359375\n",
            "Testing: [32], ReLU, Epochs=10, LR=0.001, Batch=128\n",
            "Epoch 1/10, Loss: 4645.25537109375\n",
            "Epoch 2/10, Loss: 4676.438197544643\n",
            "Epoch 3/10, Loss: 4615.369838169643\n",
            "Epoch 4/10, Loss: 4591.100795200893\n",
            "Epoch 5/10, Loss: 4570.712681361607\n",
            "Epoch 6/10, Loss: 4537.128487723215\n",
            "Epoch 7/10, Loss: 4519.556082589285\n",
            "Epoch 8/10, Loss: 4531.886579241072\n",
            "Epoch 9/10, Loss: 4512.554547991072\n",
            "Epoch 10/10, Loss: 4478.343959263393\n",
            "Testing: [32], ReLU, Epochs=50, LR=0.01, Batch=32\n",
            "Epoch 1/50, Loss: 4229.9403125\n",
            "Epoch 2/50, Loss: 2311.3931591796877\n",
            "Epoch 3/50, Loss: 362.8011264038086\n",
            "Epoch 4/50, Loss: 145.40923767089845\n",
            "Epoch 5/50, Loss: 125.22138275146484\n",
            "Epoch 6/50, Loss: 115.83259490966798\n",
            "Epoch 7/50, Loss: 107.5034814453125\n",
            "Epoch 8/50, Loss: 99.82791870117188\n",
            "Epoch 9/50, Loss: 93.12229782104492\n",
            "Epoch 10/50, Loss: 86.12967254638671\n",
            "Epoch 11/50, Loss: 80.31911056518555\n",
            "Epoch 12/50, Loss: 75.08962844848632\n",
            "Epoch 13/50, Loss: 70.26074096679687\n",
            "Epoch 14/50, Loss: 65.87510086059571\n",
            "Epoch 15/50, Loss: 61.93944786071777\n",
            "Epoch 16/50, Loss: 57.81975326538086\n",
            "Epoch 17/50, Loss: 53.9212614440918\n",
            "Epoch 18/50, Loss: 50.44320701599121\n",
            "Epoch 19/50, Loss: 45.6379866027832\n",
            "Epoch 20/50, Loss: 43.50529197692871\n",
            "Epoch 21/50, Loss: 40.54859756469727\n",
            "Epoch 22/50, Loss: 38.450757904052736\n",
            "Epoch 23/50, Loss: 36.55713321685791\n",
            "Epoch 24/50, Loss: 35.1392488861084\n",
            "Epoch 25/50, Loss: 33.73012878417969\n",
            "Epoch 26/50, Loss: 32.65001815795898\n",
            "Epoch 27/50, Loss: 31.73353988647461\n",
            "Epoch 28/50, Loss: 31.016173553466796\n",
            "Epoch 29/50, Loss: 30.6336994934082\n",
            "Epoch 30/50, Loss: 30.19027801513672\n",
            "Epoch 31/50, Loss: 30.017065963745118\n",
            "Epoch 32/50, Loss: 29.911260452270508\n",
            "Epoch 33/50, Loss: 29.110099411010744\n",
            "Epoch 34/50, Loss: 28.734092559814453\n",
            "Epoch 35/50, Loss: 29.21483383178711\n",
            "Epoch 36/50, Loss: 28.508895111083984\n",
            "Epoch 37/50, Loss: 28.480905952453615\n",
            "Epoch 38/50, Loss: 28.441288299560547\n",
            "Epoch 39/50, Loss: 28.14369445800781\n",
            "Epoch 40/50, Loss: 28.527106399536134\n",
            "Epoch 41/50, Loss: 28.313369064331056\n",
            "Epoch 42/50, Loss: 28.086857681274413\n",
            "Epoch 43/50, Loss: 28.16859130859375\n",
            "Epoch 44/50, Loss: 28.74784111022949\n",
            "Epoch 45/50, Loss: 29.22453857421875\n",
            "Epoch 46/50, Loss: 28.134282836914064\n",
            "Epoch 47/50, Loss: 28.017570877075194\n",
            "Epoch 48/50, Loss: 28.003291015625\n",
            "Epoch 49/50, Loss: 28.473198852539063\n",
            "Epoch 50/50, Loss: 28.245252227783205\n",
            "Testing: [32], ReLU, Epochs=50, LR=0.01, Batch=128\n",
            "Epoch 1/50, Loss: 4571.128696986607\n",
            "Epoch 2/50, Loss: 4399.183314732143\n",
            "Epoch 3/50, Loss: 3996.932198660714\n",
            "Epoch 4/50, Loss: 3485.733468191964\n",
            "Epoch 5/50, Loss: 2867.98779296875\n",
            "Epoch 6/50, Loss: 2093.0935407366073\n",
            "Epoch 7/50, Loss: 1343.9044799804688\n",
            "Epoch 8/50, Loss: 697.2110988071987\n",
            "Epoch 9/50, Loss: 286.8204999651228\n",
            "Epoch 10/50, Loss: 147.18062482561385\n",
            "Epoch 11/50, Loss: 163.68498011997767\n",
            "Epoch 12/50, Loss: 160.40059988839286\n",
            "Epoch 13/50, Loss: 146.51896449497767\n",
            "Epoch 14/50, Loss: 127.76684025355748\n",
            "Epoch 15/50, Loss: 125.43942478724888\n",
            "Epoch 16/50, Loss: 131.99547685895647\n",
            "Epoch 17/50, Loss: 120.23643602643695\n",
            "Epoch 18/50, Loss: 117.83866664341518\n",
            "Epoch 19/50, Loss: 113.8274405343192\n",
            "Epoch 20/50, Loss: 114.82387106759208\n",
            "Epoch 21/50, Loss: 109.6066425868443\n",
            "Epoch 22/50, Loss: 108.13535635811942\n",
            "Epoch 23/50, Loss: 108.03120531354632\n",
            "Epoch 24/50, Loss: 104.44385092599052\n",
            "Epoch 25/50, Loss: 97.28489903041294\n",
            "Epoch 26/50, Loss: 93.65113612583706\n",
            "Epoch 27/50, Loss: 100.23013414655414\n",
            "Epoch 28/50, Loss: 85.9936022077288\n",
            "Epoch 29/50, Loss: 86.82976749965123\n",
            "Epoch 30/50, Loss: 78.90190614972796\n",
            "Epoch 31/50, Loss: 83.00931004115513\n",
            "Epoch 32/50, Loss: 77.19293212890625\n",
            "Epoch 33/50, Loss: 76.71128736223493\n",
            "Epoch 34/50, Loss: 76.93912724086216\n",
            "Epoch 35/50, Loss: 67.91635240827289\n",
            "Epoch 36/50, Loss: 72.44157191685268\n",
            "Epoch 37/50, Loss: 70.96675273350307\n",
            "Epoch 38/50, Loss: 63.316341400146484\n",
            "Epoch 39/50, Loss: 62.45664051600865\n",
            "Epoch 40/50, Loss: 60.33598763602121\n",
            "Epoch 41/50, Loss: 59.12684794834682\n",
            "Epoch 42/50, Loss: 57.915999276297434\n",
            "Epoch 43/50, Loss: 55.29097148350307\n",
            "Epoch 44/50, Loss: 59.92601939610073\n",
            "Epoch 45/50, Loss: 55.876986912318635\n",
            "Epoch 46/50, Loss: 53.50622776576451\n",
            "Epoch 47/50, Loss: 49.73379407610212\n",
            "Epoch 48/50, Loss: 51.69181605747768\n",
            "Epoch 49/50, Loss: 49.245266505650115\n",
            "Epoch 50/50, Loss: 47.96844918387277\n",
            "Testing: [32], ReLU, Epochs=50, LR=0.001, Batch=32\n",
            "Epoch 1/50, Loss: 4618.623203125\n",
            "Epoch 2/50, Loss: 4564.414228515625\n",
            "Epoch 3/50, Loss: 4492.95720703125\n",
            "Epoch 4/50, Loss: 4395.312978515625\n",
            "Epoch 5/50, Loss: 4269.1747265625\n",
            "Epoch 6/50, Loss: 4113.26927734375\n",
            "Epoch 7/50, Loss: 3931.519580078125\n",
            "Epoch 8/50, Loss: 3723.3734765625\n",
            "Epoch 9/50, Loss: 3496.60966796875\n",
            "Epoch 10/50, Loss: 3250.86935546875\n",
            "Epoch 11/50, Loss: 2986.9620703125\n",
            "Epoch 12/50, Loss: 2712.13119140625\n",
            "Epoch 13/50, Loss: 2429.560146484375\n",
            "Epoch 14/50, Loss: 2149.441142578125\n",
            "Epoch 15/50, Loss: 1875.159267578125\n",
            "Epoch 16/50, Loss: 1612.7540869140626\n",
            "Epoch 17/50, Loss: 1366.16375\n",
            "Epoch 18/50, Loss: 1136.8132446289062\n",
            "Epoch 19/50, Loss: 931.9656323242187\n",
            "Epoch 20/50, Loss: 751.8028369140625\n",
            "Epoch 21/50, Loss: 600.8027709960937\n",
            "Epoch 22/50, Loss: 476.86151611328125\n",
            "Epoch 23/50, Loss: 376.7257653808594\n",
            "Epoch 24/50, Loss: 298.4396469116211\n",
            "Epoch 25/50, Loss: 240.75372619628905\n",
            "Epoch 26/50, Loss: 199.72924743652345\n",
            "Epoch 27/50, Loss: 172.2139758300781\n",
            "Epoch 28/50, Loss: 154.91677642822265\n",
            "Epoch 29/50, Loss: 144.7828010559082\n",
            "Epoch 30/50, Loss: 138.53888153076173\n",
            "Epoch 31/50, Loss: 135.0467578125\n",
            "Epoch 32/50, Loss: 132.44201751708985\n",
            "Epoch 33/50, Loss: 130.60738830566407\n",
            "Epoch 34/50, Loss: 128.84654357910156\n",
            "Epoch 35/50, Loss: 127.19008575439453\n",
            "Epoch 36/50, Loss: 125.58543334960937\n",
            "Epoch 37/50, Loss: 124.0443408203125\n",
            "Epoch 38/50, Loss: 122.52739501953126\n",
            "Epoch 39/50, Loss: 121.08086517333984\n",
            "Epoch 40/50, Loss: 119.5577767944336\n",
            "Epoch 41/50, Loss: 118.10637268066407\n",
            "Epoch 42/50, Loss: 116.70409774780273\n",
            "Epoch 43/50, Loss: 115.30287384033203\n",
            "Epoch 44/50, Loss: 113.8779379272461\n",
            "Epoch 45/50, Loss: 112.52696746826172\n",
            "Epoch 46/50, Loss: 111.21464721679688\n",
            "Epoch 47/50, Loss: 109.85593719482422\n",
            "Epoch 48/50, Loss: 108.55112869262695\n",
            "Epoch 49/50, Loss: 107.22961761474609\n",
            "Epoch 50/50, Loss: 105.93494873046875\n",
            "Testing: [32], ReLU, Epochs=50, LR=0.001, Batch=128\n",
            "Epoch 1/50, Loss: 4636.640276227678\n",
            "Epoch 2/50, Loss: 4628.810337611607\n",
            "Epoch 3/50, Loss: 4701.238351004465\n",
            "Epoch 4/50, Loss: 4674.937360491072\n",
            "Epoch 5/50, Loss: 4683.109235491072\n",
            "Epoch 6/50, Loss: 4598.17138671875\n",
            "Epoch 7/50, Loss: 4631.6484375\n",
            "Epoch 8/50, Loss: 4593.13134765625\n",
            "Epoch 9/50, Loss: 4608.456752232143\n",
            "Epoch 10/50, Loss: 4553.607142857143\n",
            "Epoch 11/50, Loss: 4584.778390066965\n",
            "Epoch 12/50, Loss: 4525.11328125\n",
            "Epoch 13/50, Loss: 4515.778250558035\n",
            "Epoch 14/50, Loss: 4526.195452008928\n",
            "Epoch 15/50, Loss: 4484.520089285715\n",
            "Epoch 16/50, Loss: 4440.308175223215\n",
            "Epoch 17/50, Loss: 4416.8046875\n",
            "Epoch 18/50, Loss: 4395.315499441965\n",
            "Epoch 19/50, Loss: 4389.192940848215\n",
            "Epoch 20/50, Loss: 4331.69873046875\n",
            "Epoch 21/50, Loss: 4236.868303571428\n",
            "Epoch 22/50, Loss: 4255.656459263393\n",
            "Epoch 23/50, Loss: 4204.637416294643\n",
            "Epoch 24/50, Loss: 4171.554827008928\n",
            "Epoch 25/50, Loss: 4102.39240373884\n",
            "Epoch 26/50, Loss: 4019.2655901227677\n",
            "Epoch 27/50, Loss: 4027.54931640625\n",
            "Epoch 28/50, Loss: 3939.745675223214\n",
            "Epoch 29/50, Loss: 3830.2777971540177\n",
            "Epoch 30/50, Loss: 3833.0978655133927\n",
            "Epoch 31/50, Loss: 3681.9340471540177\n",
            "Epoch 32/50, Loss: 3654.134591238839\n",
            "Epoch 33/50, Loss: 3625.39697265625\n",
            "Epoch 34/50, Loss: 3434.2734723772323\n",
            "Epoch 35/50, Loss: 3367.8710588727677\n",
            "Epoch 36/50, Loss: 3322.2251325334823\n",
            "Epoch 37/50, Loss: 3228.182791573661\n",
            "Epoch 38/50, Loss: 3177.317452566964\n",
            "Epoch 39/50, Loss: 3068.262451171875\n",
            "Epoch 40/50, Loss: 3008.153494698661\n",
            "Epoch 41/50, Loss: 2955.2361188616073\n",
            "Epoch 42/50, Loss: 2902.068533761161\n",
            "Epoch 43/50, Loss: 2732.0932965959823\n",
            "Epoch 44/50, Loss: 2641.3093610491073\n",
            "Epoch 45/50, Loss: 2544.691720145089\n",
            "Epoch 46/50, Loss: 2450.5777762276784\n",
            "Epoch 47/50, Loss: 2347.5375627790177\n",
            "Epoch 48/50, Loss: 2289.944580078125\n",
            "Epoch 49/50, Loss: 2164.5526994977677\n",
            "Epoch 50/50, Loss: 2088.052716936384\n",
            "Testing: [32], Sigmoid, Epochs=10, LR=0.01, Batch=32\n",
            "Epoch 1/10, Loss: 4242.7293359375\n",
            "Epoch 2/10, Loss: 3374.58126953125\n",
            "Epoch 3/10, Loss: 2412.859052734375\n",
            "Epoch 4/10, Loss: 1634.4454052734375\n",
            "Epoch 5/10, Loss: 1101.8566748046876\n",
            "Epoch 6/10, Loss: 754.8671704101563\n",
            "Epoch 7/10, Loss: 525.9277954101562\n",
            "Epoch 8/10, Loss: 380.65481140136717\n",
            "Epoch 9/10, Loss: 284.5649395751953\n",
            "Epoch 10/10, Loss: 223.8064810180664\n",
            "Testing: [32], Sigmoid, Epochs=10, LR=0.01, Batch=128\n",
            "Epoch 1/10, Loss: 4582.186593191965\n",
            "Epoch 2/10, Loss: 4377.462855747768\n",
            "Epoch 3/10, Loss: 4179.865304129465\n",
            "Epoch 4/10, Loss: 4015.5724051339284\n",
            "Epoch 5/10, Loss: 3768.028006417411\n",
            "Epoch 6/10, Loss: 3498.8230329241073\n",
            "Epoch 7/10, Loss: 3241.4015764508927\n",
            "Epoch 8/10, Loss: 2903.0289132254466\n",
            "Epoch 9/10, Loss: 2609.1776297433034\n",
            "Epoch 10/10, Loss: 2391.5081961495534\n",
            "Testing: [32], Sigmoid, Epochs=10, LR=0.001, Batch=32\n",
            "Epoch 1/10, Loss: 4549.087119140625\n",
            "Epoch 2/10, Loss: 4480.76271484375\n",
            "Epoch 3/10, Loss: 4410.299306640625\n",
            "Epoch 4/10, Loss: 4336.1894140625\n",
            "Epoch 5/10, Loss: 4257.8447265625\n",
            "Epoch 6/10, Loss: 4174.58716796875\n",
            "Epoch 7/10, Loss: 4086.6498828125\n",
            "Epoch 8/10, Loss: 3994.193271484375\n",
            "Epoch 9/10, Loss: 3897.18046875\n",
            "Epoch 10/10, Loss: 3796.647607421875\n",
            "Testing: [32], Sigmoid, Epochs=10, LR=0.001, Batch=128\n",
            "Epoch 1/10, Loss: 4688.396344866072\n",
            "Epoch 2/10, Loss: 4710.486049107143\n",
            "Epoch 3/10, Loss: 4766.966029575893\n",
            "Epoch 4/10, Loss: 4615.908203125\n",
            "Epoch 5/10, Loss: 4644.159528459822\n",
            "Epoch 6/10, Loss: 4625.868512834822\n",
            "Epoch 7/10, Loss: 4582.720354352678\n",
            "Epoch 8/10, Loss: 4552.6279296875\n",
            "Epoch 9/10, Loss: 4621.748465401785\n",
            "Epoch 10/10, Loss: 4564.207380022322\n",
            "Testing: [32], Sigmoid, Epochs=50, LR=0.01, Batch=32\n",
            "Epoch 1/50, Loss: 4322.808935546875\n",
            "Epoch 2/50, Loss: 3462.60416015625\n",
            "Epoch 3/50, Loss: 2460.9244970703126\n",
            "Epoch 4/50, Loss: 1666.8106005859374\n",
            "Epoch 5/50, Loss: 1125.9890063476562\n",
            "Epoch 6/50, Loss: 771.3712841796874\n",
            "Epoch 7/50, Loss: 539.8145971679687\n",
            "Epoch 8/50, Loss: 388.9866925048828\n",
            "Epoch 9/50, Loss: 289.57251037597655\n",
            "Epoch 10/50, Loss: 228.56921569824217\n",
            "Epoch 11/50, Loss: 188.17363677978517\n",
            "Epoch 12/50, Loss: 162.68033142089843\n",
            "Epoch 13/50, Loss: 145.12077239990234\n",
            "Epoch 14/50, Loss: 129.19123962402344\n",
            "Epoch 15/50, Loss: 115.6446029663086\n",
            "Epoch 16/50, Loss: 105.03269927978516\n",
            "Epoch 17/50, Loss: 96.39425537109375\n",
            "Epoch 18/50, Loss: 89.38633499145507\n",
            "Epoch 19/50, Loss: 83.42048156738281\n",
            "Epoch 20/50, Loss: 78.21047760009766\n",
            "Epoch 21/50, Loss: 73.4079522705078\n",
            "Epoch 22/50, Loss: 69.18442001342774\n",
            "Epoch 23/50, Loss: 65.36415885925292\n",
            "Epoch 24/50, Loss: 61.8678458404541\n",
            "Epoch 25/50, Loss: 58.5921134185791\n",
            "Epoch 26/50, Loss: 55.794913101196286\n",
            "Epoch 27/50, Loss: 53.34180892944336\n",
            "Epoch 28/50, Loss: 51.046810150146484\n",
            "Epoch 29/50, Loss: 49.06622756958008\n",
            "Epoch 30/50, Loss: 47.349719543457034\n",
            "Epoch 31/50, Loss: 45.81831275939941\n",
            "Epoch 32/50, Loss: 44.46084098815918\n",
            "Epoch 33/50, Loss: 43.157718124389646\n",
            "Epoch 34/50, Loss: 41.9374104309082\n",
            "Epoch 35/50, Loss: 41.0236279296875\n",
            "Epoch 36/50, Loss: 40.22098052978516\n",
            "Epoch 37/50, Loss: 39.186167602539065\n",
            "Epoch 38/50, Loss: 38.38681221008301\n",
            "Epoch 39/50, Loss: 37.84824043273926\n",
            "Epoch 40/50, Loss: 37.19204410552979\n",
            "Epoch 41/50, Loss: 36.54984718322754\n",
            "Epoch 42/50, Loss: 36.00642471313476\n",
            "Epoch 43/50, Loss: 35.52391311645508\n",
            "Epoch 44/50, Loss: 35.00193061828613\n",
            "Epoch 45/50, Loss: 34.6911799621582\n",
            "Epoch 46/50, Loss: 34.19059295654297\n",
            "Epoch 47/50, Loss: 33.827220077514646\n",
            "Epoch 48/50, Loss: 33.526159133911136\n",
            "Epoch 49/50, Loss: 33.281140899658205\n",
            "Epoch 50/50, Loss: 33.01818054199219\n",
            "Testing: [32], Sigmoid, Epochs=50, LR=0.01, Batch=128\n",
            "Epoch 1/50, Loss: 4598.271693638393\n",
            "Epoch 2/50, Loss: 4402.907924107143\n",
            "Epoch 3/50, Loss: 4219.329031808035\n",
            "Epoch 4/50, Loss: 4009.5787179129466\n",
            "Epoch 5/50, Loss: 3771.626220703125\n",
            "Epoch 6/50, Loss: 3487.59375\n",
            "Epoch 7/50, Loss: 3209.3180803571427\n",
            "Epoch 8/50, Loss: 2921.6767229352677\n",
            "Epoch 9/50, Loss: 2576.062744140625\n",
            "Epoch 10/50, Loss: 2382.9320940290177\n",
            "Epoch 11/50, Loss: 2109.1868024553573\n",
            "Epoch 12/50, Loss: 1887.61474609375\n",
            "Epoch 13/50, Loss: 1643.869419642857\n",
            "Epoch 14/50, Loss: 1522.4990059988838\n",
            "Epoch 15/50, Loss: 1351.1712995256696\n",
            "Epoch 16/50, Loss: 1177.5100620814733\n",
            "Epoch 17/50, Loss: 1053.9593331473213\n",
            "Epoch 18/50, Loss: 965.799333844866\n",
            "Epoch 19/50, Loss: 882.3035627092634\n",
            "Epoch 20/50, Loss: 805.2432163783482\n",
            "Epoch 21/50, Loss: 707.7720598493304\n",
            "Epoch 22/50, Loss: 623.1021902901786\n",
            "Epoch 23/50, Loss: 577.1773071289062\n",
            "Epoch 24/50, Loss: 513.6492178780692\n",
            "Epoch 25/50, Loss: 473.1613551548549\n",
            "Epoch 26/50, Loss: 440.0745936802455\n",
            "Epoch 27/50, Loss: 389.6350620814732\n",
            "Epoch 28/50, Loss: 356.5603986467634\n",
            "Epoch 29/50, Loss: 337.4370771135603\n",
            "Epoch 30/50, Loss: 315.90887451171875\n",
            "Epoch 31/50, Loss: 289.07293919154574\n",
            "Epoch 32/50, Loss: 265.64447239467074\n",
            "Epoch 33/50, Loss: 254.88611711774553\n",
            "Epoch 34/50, Loss: 237.12474278041296\n",
            "Epoch 35/50, Loss: 220.19017464773995\n",
            "Epoch 36/50, Loss: 229.66661507742745\n",
            "Epoch 37/50, Loss: 186.82556915283203\n",
            "Epoch 38/50, Loss: 193.6835174560547\n",
            "Epoch 39/50, Loss: 175.23110961914062\n",
            "Epoch 40/50, Loss: 168.48812648228235\n",
            "Epoch 41/50, Loss: 167.09879629952567\n",
            "Epoch 42/50, Loss: 153.37998744419642\n",
            "Epoch 43/50, Loss: 147.24134063720703\n",
            "Epoch 44/50, Loss: 145.19176047188895\n",
            "Epoch 45/50, Loss: 144.09835488455636\n",
            "Epoch 46/50, Loss: 142.83824157714844\n",
            "Epoch 47/50, Loss: 128.3550306047712\n",
            "Epoch 48/50, Loss: 128.8464126586914\n",
            "Epoch 49/50, Loss: 123.90446472167969\n",
            "Epoch 50/50, Loss: 123.88171495710101\n",
            "Testing: [32], Sigmoid, Epochs=50, LR=0.001, Batch=32\n",
            "Epoch 1/50, Loss: 4667.74353515625\n",
            "Epoch 2/50, Loss: 4599.5341015625\n",
            "Epoch 3/50, Loss: 4532.121513671875\n",
            "Epoch 4/50, Loss: 4463.217138671875\n",
            "Epoch 5/50, Loss: 4391.765283203125\n",
            "Epoch 6/50, Loss: 4317.369951171875\n",
            "Epoch 7/50, Loss: 4239.059931640625\n",
            "Epoch 8/50, Loss: 4156.326435546875\n",
            "Epoch 9/50, Loss: 4069.130302734375\n",
            "Epoch 10/50, Loss: 3977.306494140625\n",
            "Epoch 11/50, Loss: 3880.294140625\n",
            "Epoch 12/50, Loss: 3778.59287109375\n",
            "Epoch 13/50, Loss: 3672.9184375\n",
            "Epoch 14/50, Loss: 3563.812509765625\n",
            "Epoch 15/50, Loss: 3452.986416015625\n",
            "Epoch 16/50, Loss: 3341.129541015625\n",
            "Epoch 17/50, Loss: 3230.367265625\n",
            "Epoch 18/50, Loss: 3120.5872265625\n",
            "Epoch 19/50, Loss: 3013.114501953125\n",
            "Epoch 20/50, Loss: 2908.5847265625\n",
            "Epoch 21/50, Loss: 2807.350224609375\n",
            "Epoch 22/50, Loss: 2708.834931640625\n",
            "Epoch 23/50, Loss: 2614.139794921875\n",
            "Epoch 24/50, Loss: 2522.31849609375\n",
            "Epoch 25/50, Loss: 2433.9566455078125\n",
            "Epoch 26/50, Loss: 2348.661083984375\n",
            "Epoch 27/50, Loss: 2266.84232421875\n",
            "Epoch 28/50, Loss: 2187.4695703125\n",
            "Epoch 29/50, Loss: 2111.2062353515626\n",
            "Epoch 30/50, Loss: 2037.509306640625\n",
            "Epoch 31/50, Loss: 1966.738408203125\n",
            "Epoch 32/50, Loss: 1898.3146142578125\n",
            "Epoch 33/50, Loss: 1832.2540966796876\n",
            "Epoch 34/50, Loss: 1768.6642041015625\n",
            "Epoch 35/50, Loss: 1707.2507763671874\n",
            "Epoch 36/50, Loss: 1648.07548828125\n",
            "Epoch 37/50, Loss: 1590.5908740234374\n",
            "Epoch 38/50, Loss: 1535.4467822265624\n",
            "Epoch 39/50, Loss: 1482.463837890625\n",
            "Epoch 40/50, Loss: 1430.8045971679687\n",
            "Epoch 41/50, Loss: 1381.11515625\n",
            "Epoch 42/50, Loss: 1333.15275390625\n",
            "Epoch 43/50, Loss: 1287.024443359375\n",
            "Epoch 44/50, Loss: 1242.2580346679688\n",
            "Epoch 45/50, Loss: 1199.033193359375\n",
            "Epoch 46/50, Loss: 1157.6481958007812\n",
            "Epoch 47/50, Loss: 1117.181103515625\n",
            "Epoch 48/50, Loss: 1078.5567944335937\n",
            "Epoch 49/50, Loss: 1041.0856884765626\n",
            "Epoch 50/50, Loss: 1004.98490234375\n",
            "Testing: [32], Sigmoid, Epochs=50, LR=0.001, Batch=128\n",
            "Epoch 1/50, Loss: 4612.060616629465\n",
            "Epoch 2/50, Loss: 4558.563685825893\n",
            "Epoch 3/50, Loss: 4578.664969308035\n",
            "Epoch 4/50, Loss: 4570.59814453125\n",
            "Epoch 5/50, Loss: 4559.189034598215\n",
            "Epoch 6/50, Loss: 4509.245396205357\n",
            "Epoch 7/50, Loss: 4544.854282924107\n",
            "Epoch 8/50, Loss: 4543.489467075893\n",
            "Epoch 9/50, Loss: 4462.86669921875\n",
            "Epoch 10/50, Loss: 4464.523018973215\n",
            "Epoch 11/50, Loss: 4428.071149553572\n",
            "Epoch 12/50, Loss: 4554.040806361607\n",
            "Epoch 13/50, Loss: 4346.573869977678\n",
            "Epoch 14/50, Loss: 4340.562534877232\n",
            "Epoch 15/50, Loss: 4288.636300223215\n",
            "Epoch 16/50, Loss: 4402.758021763393\n",
            "Epoch 17/50, Loss: 4299.873116629465\n",
            "Epoch 18/50, Loss: 4316.351981026785\n",
            "Epoch 19/50, Loss: 4316.091657366072\n",
            "Epoch 20/50, Loss: 4239.800885881697\n",
            "Epoch 21/50, Loss: 4217.060511997768\n",
            "Epoch 22/50, Loss: 4158.464599609375\n",
            "Epoch 23/50, Loss: 4152.56951032366\n",
            "Epoch 24/50, Loss: 4139.488664899553\n",
            "Epoch 25/50, Loss: 4137.336635044643\n",
            "Epoch 26/50, Loss: 4131.464460100447\n",
            "Epoch 27/50, Loss: 4059.3179059709823\n",
            "Epoch 28/50, Loss: 4026.292654854911\n",
            "Epoch 29/50, Loss: 4030.37744140625\n",
            "Epoch 30/50, Loss: 3982.3169991629466\n",
            "Epoch 31/50, Loss: 3968.499441964286\n",
            "Epoch 32/50, Loss: 3937.1076311383927\n",
            "Epoch 33/50, Loss: 3849.0185198102677\n",
            "Epoch 34/50, Loss: 3867.157156808036\n",
            "Epoch 35/50, Loss: 3831.1828962053573\n",
            "Epoch 36/50, Loss: 3817.0518624441966\n",
            "Epoch 37/50, Loss: 3779.954031808036\n",
            "Epoch 38/50, Loss: 3768.445731026786\n",
            "Epoch 39/50, Loss: 3667.4751325334823\n",
            "Epoch 40/50, Loss: 3649.420166015625\n",
            "Epoch 41/50, Loss: 3671.0363420758927\n",
            "Epoch 42/50, Loss: 3605.411865234375\n",
            "Epoch 43/50, Loss: 3571.7022530691966\n",
            "Epoch 44/50, Loss: 3601.469552176339\n",
            "Epoch 45/50, Loss: 3568.832345145089\n",
            "Epoch 46/50, Loss: 3519.824881417411\n",
            "Epoch 47/50, Loss: 3472.3985421316966\n",
            "Epoch 48/50, Loss: 3430.249825613839\n",
            "Epoch 49/50, Loss: 3422.3045828683034\n",
            "Epoch 50/50, Loss: 3330.0601632254466\n",
            "Testing: [32, 16], ReLU, Epochs=10, LR=0.01, Batch=32\n",
            "Epoch 1/10, Loss: 3805.13177734375\n",
            "Epoch 2/10, Loss: 401.25529022216796\n",
            "Epoch 3/10, Loss: 134.37547729492186\n",
            "Epoch 4/10, Loss: 89.4483415222168\n",
            "Epoch 5/10, Loss: 78.65142852783202\n",
            "Epoch 6/10, Loss: 72.3178321838379\n",
            "Epoch 7/10, Loss: 65.60076568603516\n",
            "Epoch 8/10, Loss: 61.39484161376953\n",
            "Epoch 9/10, Loss: 57.59196113586426\n",
            "Epoch 10/10, Loss: 51.4527734375\n",
            "Testing: [32, 16], ReLU, Epochs=10, LR=0.01, Batch=128\n",
            "Epoch 1/10, Loss: 4601.836146763393\n",
            "Epoch 2/10, Loss: 4328.271728515625\n",
            "Epoch 3/10, Loss: 3608.7110770089284\n",
            "Epoch 4/10, Loss: 2116.5155203683034\n",
            "Epoch 5/10, Loss: 544.0650743756976\n",
            "Epoch 6/10, Loss: 415.1876743861607\n",
            "Epoch 7/10, Loss: 191.16986519949776\n",
            "Epoch 8/10, Loss: 175.42982700892858\n",
            "Epoch 9/10, Loss: 152.79671587262834\n",
            "Epoch 10/10, Loss: 110.91722760881696\n",
            "Testing: [32, 16], ReLU, Epochs=10, LR=0.001, Batch=32\n",
            "Epoch 1/10, Loss: 4652.656875\n",
            "Epoch 2/10, Loss: 4589.6927734375\n",
            "Epoch 3/10, Loss: 4428.136318359375\n",
            "Epoch 4/10, Loss: 4087.081357421875\n",
            "Epoch 5/10, Loss: 3469.114296875\n",
            "Epoch 6/10, Loss: 2563.052958984375\n",
            "Epoch 7/10, Loss: 1520.9277465820312\n",
            "Epoch 8/10, Loss: 654.0563916015625\n",
            "Epoch 9/10, Loss: 222.86937561035157\n",
            "Epoch 10/10, Loss: 126.69324157714844\n",
            "Testing: [32, 16], ReLU, Epochs=10, LR=0.001, Batch=128\n",
            "Epoch 1/10, Loss: 4628.404366629465\n",
            "Epoch 2/10, Loss: 4582.479701450893\n",
            "Epoch 3/10, Loss: 4627.340471540178\n",
            "Epoch 4/10, Loss: 4567.978236607143\n",
            "Epoch 5/10, Loss: 4626.037109375\n",
            "Epoch 6/10, Loss: 4536.844656808035\n",
            "Epoch 7/10, Loss: 4559.525251116072\n",
            "Epoch 8/10, Loss: 4457.384381975447\n",
            "Epoch 9/10, Loss: 4516.217354910715\n",
            "Epoch 10/10, Loss: 4465.388253348215\n",
            "Testing: [32, 16], ReLU, Epochs=50, LR=0.01, Batch=32\n",
            "Epoch 1/50, Loss: 4029.935498046875\n",
            "Epoch 2/50, Loss: 632.7083209228516\n",
            "Epoch 3/50, Loss: 135.52766494750978\n",
            "Epoch 4/50, Loss: 98.40472717285157\n",
            "Epoch 5/50, Loss: 86.00671325683594\n",
            "Epoch 6/50, Loss: 76.72688705444335\n",
            "Epoch 7/50, Loss: 69.79520141601563\n",
            "Epoch 8/50, Loss: 63.720115661621094\n",
            "Epoch 9/50, Loss: 57.8958283996582\n",
            "Epoch 10/50, Loss: 54.489063110351566\n",
            "Epoch 11/50, Loss: 50.68827140808106\n",
            "Epoch 12/50, Loss: 47.25363563537598\n",
            "Epoch 13/50, Loss: 44.2309635925293\n",
            "Epoch 14/50, Loss: 41.17161117553711\n",
            "Epoch 15/50, Loss: 38.48751693725586\n",
            "Epoch 16/50, Loss: 38.189186553955075\n",
            "Epoch 17/50, Loss: 34.64493179321289\n",
            "Epoch 18/50, Loss: 33.6844518661499\n",
            "Epoch 19/50, Loss: 32.12182693481445\n",
            "Epoch 20/50, Loss: 31.8334468460083\n",
            "Epoch 21/50, Loss: 30.37395477294922\n",
            "Epoch 22/50, Loss: 29.39357536315918\n",
            "Epoch 23/50, Loss: 29.703792839050294\n",
            "Epoch 24/50, Loss: 28.797486419677735\n",
            "Epoch 25/50, Loss: 28.78196044921875\n",
            "Epoch 26/50, Loss: 29.383223190307618\n",
            "Epoch 27/50, Loss: 28.862337646484374\n",
            "Epoch 28/50, Loss: 28.329659042358397\n",
            "Epoch 29/50, Loss: 28.03298957824707\n",
            "Epoch 30/50, Loss: 28.31176612854004\n",
            "Epoch 31/50, Loss: 27.92623119354248\n",
            "Epoch 32/50, Loss: 28.176552658081054\n",
            "Epoch 33/50, Loss: 28.033853302001955\n",
            "Epoch 34/50, Loss: 28.04617660522461\n",
            "Epoch 35/50, Loss: 27.72938705444336\n",
            "Epoch 36/50, Loss: 27.493739776611328\n",
            "Epoch 37/50, Loss: 27.768925704956054\n",
            "Epoch 38/50, Loss: 29.182425689697265\n",
            "Epoch 39/50, Loss: 29.55586078643799\n",
            "Epoch 40/50, Loss: 27.62950382232666\n",
            "Epoch 41/50, Loss: 27.13474868774414\n",
            "Epoch 42/50, Loss: 26.967314949035643\n",
            "Epoch 43/50, Loss: 27.53478530883789\n",
            "Epoch 44/50, Loss: 27.227192306518553\n",
            "Epoch 45/50, Loss: 28.15077625274658\n",
            "Epoch 46/50, Loss: 28.9305126953125\n",
            "Epoch 47/50, Loss: 27.462992362976074\n",
            "Epoch 48/50, Loss: 27.0007767868042\n",
            "Epoch 49/50, Loss: 26.95255516052246\n",
            "Epoch 50/50, Loss: 28.037820014953613\n",
            "Testing: [32, 16], ReLU, Epochs=50, LR=0.01, Batch=128\n",
            "Epoch 1/50, Loss: 4594.015485491072\n",
            "Epoch 2/50, Loss: 4473.740443638393\n",
            "Epoch 3/50, Loss: 3941.4469866071427\n",
            "Epoch 4/50, Loss: 2688.5223214285716\n",
            "Epoch 5/50, Loss: 926.381365094866\n",
            "Epoch 6/50, Loss: 261.14574650355746\n",
            "Epoch 7/50, Loss: 334.1926792689732\n",
            "Epoch 8/50, Loss: 145.8805204119001\n",
            "Epoch 9/50, Loss: 175.39239937918526\n",
            "Epoch 10/50, Loss: 113.95974513462612\n",
            "Epoch 11/50, Loss: 111.58153642926898\n",
            "Epoch 12/50, Loss: 107.45912279401507\n",
            "Epoch 13/50, Loss: 98.06515175955636\n",
            "Epoch 14/50, Loss: 91.66589028494698\n",
            "Epoch 15/50, Loss: 97.43824768066406\n",
            "Epoch 16/50, Loss: 89.04860033307757\n",
            "Epoch 17/50, Loss: 85.55981118338448\n",
            "Epoch 18/50, Loss: 83.80880519321987\n",
            "Epoch 19/50, Loss: 81.91134534563336\n",
            "Epoch 20/50, Loss: 74.3683613368443\n",
            "Epoch 21/50, Loss: 71.26120213099888\n",
            "Epoch 22/50, Loss: 71.45113917759487\n",
            "Epoch 23/50, Loss: 71.24782998221261\n",
            "Epoch 24/50, Loss: 70.26031930106026\n",
            "Epoch 25/50, Loss: 63.42705862862723\n",
            "Epoch 26/50, Loss: 62.20961543491909\n",
            "Epoch 27/50, Loss: 59.64033399309431\n",
            "Epoch 28/50, Loss: 58.088844299316406\n",
            "Epoch 29/50, Loss: 54.05199868338449\n",
            "Epoch 30/50, Loss: 51.42167282104492\n",
            "Epoch 31/50, Loss: 54.31979315621512\n",
            "Epoch 32/50, Loss: 48.80873434884207\n",
            "Epoch 33/50, Loss: 49.46960231236049\n",
            "Epoch 34/50, Loss: 48.187583923339844\n",
            "Epoch 35/50, Loss: 43.39777537754604\n",
            "Epoch 36/50, Loss: 43.57099533081055\n",
            "Epoch 37/50, Loss: 44.048587799072266\n",
            "Epoch 38/50, Loss: 40.11454445975168\n",
            "Epoch 39/50, Loss: 39.79341833932059\n",
            "Epoch 40/50, Loss: 40.10800443376814\n",
            "Epoch 41/50, Loss: 36.86908476693289\n",
            "Epoch 42/50, Loss: 39.01262937273298\n",
            "Epoch 43/50, Loss: 37.33722169058664\n",
            "Epoch 44/50, Loss: 35.19999885559082\n",
            "Epoch 45/50, Loss: 34.365551812308176\n",
            "Epoch 46/50, Loss: 34.473391669137136\n",
            "Epoch 47/50, Loss: 33.80536133902414\n",
            "Epoch 48/50, Loss: 31.93439211164202\n",
            "Epoch 49/50, Loss: 32.16434260777065\n",
            "Epoch 50/50, Loss: 31.021822248186385\n",
            "Testing: [32, 16], ReLU, Epochs=50, LR=0.001, Batch=32\n",
            "Epoch 1/50, Loss: 4656.000791015625\n",
            "Epoch 2/50, Loss: 4600.148564453125\n",
            "Epoch 3/50, Loss: 4468.12080078125\n",
            "Epoch 4/50, Loss: 4161.364521484375\n",
            "Epoch 5/50, Loss: 3566.588466796875\n",
            "Epoch 6/50, Loss: 2661.0054150390624\n",
            "Epoch 7/50, Loss: 1582.218232421875\n",
            "Epoch 8/50, Loss: 681.9086724853515\n",
            "Epoch 9/50, Loss: 227.8981640625\n",
            "Epoch 10/50, Loss: 128.983154296875\n",
            "Epoch 11/50, Loss: 121.93006378173828\n",
            "Epoch 12/50, Loss: 118.45282867431641\n",
            "Epoch 13/50, Loss: 115.80730590820312\n",
            "Epoch 14/50, Loss: 113.24429183959961\n",
            "Epoch 15/50, Loss: 110.50919525146485\n",
            "Epoch 16/50, Loss: 108.06797485351562\n",
            "Epoch 17/50, Loss: 105.67840270996093\n",
            "Epoch 18/50, Loss: 103.39764389038086\n",
            "Epoch 19/50, Loss: 101.24064071655273\n",
            "Epoch 20/50, Loss: 99.23232818603516\n",
            "Epoch 21/50, Loss: 97.18949264526367\n",
            "Epoch 22/50, Loss: 95.2725765991211\n",
            "Epoch 23/50, Loss: 93.62469696044921\n",
            "Epoch 24/50, Loss: 91.79841339111329\n",
            "Epoch 25/50, Loss: 89.88092590332032\n",
            "Epoch 26/50, Loss: 88.33315689086913\n",
            "Epoch 27/50, Loss: 86.57605484008789\n",
            "Epoch 28/50, Loss: 85.2971516418457\n",
            "Epoch 29/50, Loss: 83.53742340087891\n",
            "Epoch 30/50, Loss: 82.1221710205078\n",
            "Epoch 31/50, Loss: 80.66835235595703\n",
            "Epoch 32/50, Loss: 79.35000045776367\n",
            "Epoch 33/50, Loss: 77.89119216918945\n",
            "Epoch 34/50, Loss: 76.62935821533203\n",
            "Epoch 35/50, Loss: 75.55454971313476\n",
            "Epoch 36/50, Loss: 74.18248733520508\n",
            "Epoch 37/50, Loss: 72.99833847045899\n",
            "Epoch 38/50, Loss: 71.6297769165039\n",
            "Epoch 39/50, Loss: 70.59409454345703\n",
            "Epoch 40/50, Loss: 69.43464553833007\n",
            "Epoch 41/50, Loss: 68.39979690551758\n",
            "Epoch 42/50, Loss: 67.40498519897461\n",
            "Epoch 43/50, Loss: 66.31405578613281\n",
            "Epoch 44/50, Loss: 65.23876678466797\n",
            "Epoch 45/50, Loss: 64.42841262817383\n",
            "Epoch 46/50, Loss: 63.321797637939454\n",
            "Epoch 47/50, Loss: 62.32632827758789\n",
            "Epoch 48/50, Loss: 61.357108459472656\n",
            "Epoch 49/50, Loss: 60.49975402832031\n",
            "Epoch 50/50, Loss: 59.57954399108887\n",
            "Testing: [32, 16], ReLU, Epochs=50, LR=0.001, Batch=128\n",
            "Epoch 1/50, Loss: 4671.673270089285\n",
            "Epoch 2/50, Loss: 4668.313197544643\n",
            "Epoch 3/50, Loss: 4568.938127790178\n",
            "Epoch 4/50, Loss: 4617.043317522322\n",
            "Epoch 5/50, Loss: 4587.187848772322\n",
            "Epoch 6/50, Loss: 4472.436732700893\n",
            "Epoch 7/50, Loss: 4382.836042131697\n",
            "Epoch 8/50, Loss: 4501.297642299107\n",
            "Epoch 9/50, Loss: 4375.251953125\n",
            "Epoch 10/50, Loss: 4245.507882254465\n",
            "Epoch 11/50, Loss: 4154.2041015625\n",
            "Epoch 12/50, Loss: 4041.383475167411\n",
            "Epoch 13/50, Loss: 3939.041817801339\n",
            "Epoch 14/50, Loss: 3766.925467354911\n",
            "Epoch 15/50, Loss: 3540.6816755022323\n",
            "Epoch 16/50, Loss: 3272.477608816964\n",
            "Epoch 17/50, Loss: 3060.191580636161\n",
            "Epoch 18/50, Loss: 2793.335623604911\n",
            "Epoch 19/50, Loss: 2509.8740234375\n",
            "Epoch 20/50, Loss: 2176.4058140345983\n",
            "Epoch 21/50, Loss: 1876.28515625\n",
            "Epoch 22/50, Loss: 1566.127650669643\n",
            "Epoch 23/50, Loss: 1233.2892368861608\n",
            "Epoch 24/50, Loss: 1003.7624250139509\n",
            "Epoch 25/50, Loss: 761.3582850864956\n",
            "Epoch 26/50, Loss: 549.5243094308036\n",
            "Epoch 27/50, Loss: 404.47814505440846\n",
            "Epoch 28/50, Loss: 289.1042742047991\n",
            "Epoch 29/50, Loss: 217.05659920828683\n",
            "Epoch 30/50, Loss: 167.41034153529577\n",
            "Epoch 31/50, Loss: 158.28458731515067\n",
            "Epoch 32/50, Loss: 141.04747009277344\n",
            "Epoch 33/50, Loss: 145.2905545915876\n",
            "Epoch 34/50, Loss: 133.2398180280413\n",
            "Epoch 35/50, Loss: 133.71743229457311\n",
            "Epoch 36/50, Loss: 135.16061183384485\n",
            "Epoch 37/50, Loss: 129.84589168003626\n",
            "Epoch 38/50, Loss: 126.44888959612165\n",
            "Epoch 39/50, Loss: 135.75813947405135\n",
            "Epoch 40/50, Loss: 127.53946031842914\n",
            "Epoch 41/50, Loss: 137.1626739501953\n",
            "Epoch 42/50, Loss: 131.21869223458427\n",
            "Epoch 43/50, Loss: 129.59619794573103\n",
            "Epoch 44/50, Loss: 127.15609087262835\n",
            "Epoch 45/50, Loss: 129.77739715576172\n",
            "Epoch 46/50, Loss: 122.76288713727679\n",
            "Epoch 47/50, Loss: 120.73679460797992\n",
            "Epoch 48/50, Loss: 123.40122331891742\n",
            "Epoch 49/50, Loss: 119.18515014648438\n",
            "Epoch 50/50, Loss: 123.02188873291016\n",
            "Testing: [32, 16], Sigmoid, Epochs=10, LR=0.01, Batch=32\n",
            "Epoch 1/10, Loss: 4497.69904296875\n",
            "Epoch 2/10, Loss: 4076.134365234375\n",
            "Epoch 3/10, Loss: 3673.497685546875\n",
            "Epoch 4/10, Loss: 3315.702763671875\n",
            "Epoch 5/10, Loss: 2994.441611328125\n",
            "Epoch 6/10, Loss: 2701.627978515625\n",
            "Epoch 7/10, Loss: 2385.0317724609376\n",
            "Epoch 8/10, Loss: 2029.595654296875\n",
            "Epoch 9/10, Loss: 1702.0520849609375\n",
            "Epoch 10/10, Loss: 1424.2821533203125\n",
            "Testing: [32, 16], Sigmoid, Epochs=10, LR=0.01, Batch=128\n",
            "Epoch 1/10, Loss: 4652.675432477678\n",
            "Epoch 2/10, Loss: 4437.977260044643\n",
            "Epoch 3/10, Loss: 4299.19939313616\n",
            "Epoch 4/10, Loss: 4196.788818359375\n",
            "Epoch 5/10, Loss: 4104.331019810268\n",
            "Epoch 6/10, Loss: 4039.264474051339\n",
            "Epoch 7/10, Loss: 3966.787423270089\n",
            "Epoch 8/10, Loss: 3805.585274832589\n",
            "Epoch 9/10, Loss: 3687.5006277901784\n",
            "Epoch 10/10, Loss: 3643.5455496651784\n",
            "Testing: [32, 16], Sigmoid, Epochs=10, LR=0.001, Batch=32\n",
            "Epoch 1/10, Loss: 4571.39001953125\n",
            "Epoch 2/10, Loss: 4508.6292578125\n",
            "Epoch 3/10, Loss: 4444.168564453125\n",
            "Epoch 4/10, Loss: 4377.9903515625\n",
            "Epoch 5/10, Loss: 4313.46333984375\n",
            "Epoch 6/10, Loss: 4253.458251953125\n",
            "Epoch 7/10, Loss: 4198.570615234375\n",
            "Epoch 8/10, Loss: 4145.6352734375\n",
            "Epoch 9/10, Loss: 4092.65365234375\n",
            "Epoch 10/10, Loss: 4041.281962890625\n",
            "Testing: [32, 16], Sigmoid, Epochs=10, LR=0.001, Batch=128\n",
            "Epoch 1/10, Loss: 4661.896414620535\n",
            "Epoch 2/10, Loss: 4601.259416852678\n",
            "Epoch 3/10, Loss: 4612.365583147322\n",
            "Epoch 4/10, Loss: 4566.871233258928\n",
            "Epoch 5/10, Loss: 4592.63654436384\n",
            "Epoch 6/10, Loss: 4540.663434709822\n",
            "Epoch 7/10, Loss: 4592.934640066965\n",
            "Epoch 8/10, Loss: 4545.554757254465\n",
            "Epoch 9/10, Loss: 4469.270577566965\n",
            "Epoch 10/10, Loss: 4502.326171875\n",
            "Testing: [32, 16], Sigmoid, Epochs=50, LR=0.01, Batch=32\n",
            "Epoch 1/50, Loss: 4308.93685546875\n",
            "Epoch 2/50, Loss: 3818.56337890625\n",
            "Epoch 3/50, Loss: 3365.988564453125\n",
            "Epoch 4/50, Loss: 2974.89009765625\n",
            "Epoch 5/50, Loss: 2617.989892578125\n",
            "Epoch 6/50, Loss: 2255.5094189453125\n",
            "Epoch 7/50, Loss: 1908.53478515625\n",
            "Epoch 8/50, Loss: 1617.5971728515624\n",
            "Epoch 9/50, Loss: 1376.619833984375\n",
            "Epoch 10/50, Loss: 1146.7911059570313\n",
            "Epoch 11/50, Loss: 946.867216796875\n",
            "Epoch 12/50, Loss: 791.2578491210937\n",
            "Epoch 13/50, Loss: 667.1464904785156\n",
            "Epoch 14/50, Loss: 568.6722619628906\n",
            "Epoch 15/50, Loss: 490.4753259277344\n",
            "Epoch 16/50, Loss: 428.085087890625\n",
            "Epoch 17/50, Loss: 379.5910968017578\n",
            "Epoch 18/50, Loss: 331.4047393798828\n",
            "Epoch 19/50, Loss: 284.0912194824219\n",
            "Epoch 20/50, Loss: 249.28371490478514\n",
            "Epoch 21/50, Loss: 220.19458465576173\n",
            "Epoch 22/50, Loss: 196.1018487548828\n",
            "Epoch 23/50, Loss: 175.6012762451172\n",
            "Epoch 24/50, Loss: 159.50112518310547\n",
            "Epoch 25/50, Loss: 144.71003356933593\n",
            "Epoch 26/50, Loss: 132.2827082824707\n",
            "Epoch 27/50, Loss: 122.11599090576172\n",
            "Epoch 28/50, Loss: 113.3094953918457\n",
            "Epoch 29/50, Loss: 105.67626617431641\n",
            "Epoch 30/50, Loss: 99.08924629211425\n",
            "Epoch 31/50, Loss: 93.35024124145508\n",
            "Epoch 32/50, Loss: 88.14023254394532\n",
            "Epoch 33/50, Loss: 83.76395629882812\n",
            "Epoch 34/50, Loss: 79.4708038330078\n",
            "Epoch 35/50, Loss: 75.79668502807617\n",
            "Epoch 36/50, Loss: 72.74943481445312\n",
            "Epoch 37/50, Loss: 69.13188873291016\n",
            "Epoch 38/50, Loss: 67.21418640136719\n",
            "Epoch 39/50, Loss: 63.767243194580075\n",
            "Epoch 40/50, Loss: 60.695967483520505\n",
            "Epoch 41/50, Loss: 58.96392616271973\n",
            "Epoch 42/50, Loss: 56.71187431335449\n",
            "Epoch 43/50, Loss: 54.928399810791014\n",
            "Epoch 44/50, Loss: 52.63288345336914\n",
            "Epoch 45/50, Loss: 51.198397369384764\n",
            "Epoch 46/50, Loss: 49.4290633392334\n",
            "Epoch 47/50, Loss: 48.1222191619873\n",
            "Epoch 48/50, Loss: 47.54622688293457\n",
            "Epoch 49/50, Loss: 46.11497077941895\n",
            "Epoch 50/50, Loss: 44.693282012939456\n",
            "Testing: [32, 16], Sigmoid, Epochs=50, LR=0.01, Batch=128\n",
            "Epoch 1/50, Loss: 4612.166015625\n",
            "Epoch 2/50, Loss: 4424.063337053572\n",
            "Epoch 3/50, Loss: 4266.957240513393\n",
            "Epoch 4/50, Loss: 4171.606968470982\n",
            "Epoch 5/50, Loss: 4059.2265276227677\n",
            "Epoch 6/50, Loss: 3877.2362932477677\n",
            "Epoch 7/50, Loss: 3812.3010602678573\n",
            "Epoch 8/50, Loss: 3665.313162667411\n",
            "Epoch 9/50, Loss: 3554.3537248883927\n",
            "Epoch 10/50, Loss: 3425.956124441964\n",
            "Epoch 11/50, Loss: 3303.9716448102677\n",
            "Epoch 12/50, Loss: 3259.2358747209823\n",
            "Epoch 13/50, Loss: 3043.205636160714\n",
            "Epoch 14/50, Loss: 2948.9927106584823\n",
            "Epoch 15/50, Loss: 2790.2598702566966\n",
            "Epoch 16/50, Loss: 2693.4228864397323\n",
            "Epoch 17/50, Loss: 2598.2208077566966\n",
            "Epoch 18/50, Loss: 2511.4879324776784\n",
            "Epoch 19/50, Loss: 2425.9383370535716\n",
            "Epoch 20/50, Loss: 2293.2297712053573\n",
            "Epoch 21/50, Loss: 2225.6632080078125\n",
            "Epoch 22/50, Loss: 2112.3467145647323\n",
            "Epoch 23/50, Loss: 2048.816476004464\n",
            "Epoch 24/50, Loss: 2016.682407924107\n",
            "Epoch 25/50, Loss: 1864.9918038504463\n",
            "Epoch 26/50, Loss: 1803.4117954799108\n",
            "Epoch 27/50, Loss: 1739.5674874441963\n",
            "Epoch 28/50, Loss: 1679.4349539620537\n",
            "Epoch 29/50, Loss: 1594.2397635323662\n",
            "Epoch 30/50, Loss: 1521.2713623046875\n",
            "Epoch 31/50, Loss: 1477.5355224609375\n",
            "Epoch 32/50, Loss: 1389.7561907087054\n",
            "Epoch 33/50, Loss: 1365.0646449497767\n",
            "Epoch 34/50, Loss: 1261.2592075892858\n",
            "Epoch 35/50, Loss: 1183.656938825335\n",
            "Epoch 36/50, Loss: 1160.3755754743304\n",
            "Epoch 37/50, Loss: 1115.3458077566963\n",
            "Epoch 38/50, Loss: 1054.7542811802455\n",
            "Epoch 39/50, Loss: 1014.18310546875\n",
            "Epoch 40/50, Loss: 972.132306780134\n",
            "Epoch 41/50, Loss: 946.6124790736607\n",
            "Epoch 42/50, Loss: 867.304949079241\n",
            "Epoch 43/50, Loss: 842.8247942243304\n",
            "Epoch 44/50, Loss: 781.6039515904018\n",
            "Epoch 45/50, Loss: 777.8446132114956\n",
            "Epoch 46/50, Loss: 739.7434953962054\n",
            "Epoch 47/50, Loss: 705.4054565429688\n",
            "Epoch 48/50, Loss: 672.7322474888393\n",
            "Epoch 49/50, Loss: 641.0312848772321\n",
            "Epoch 50/50, Loss: 623.3748779296875\n",
            "Testing: [32, 16], Sigmoid, Epochs=50, LR=0.001, Batch=32\n",
            "Epoch 1/50, Loss: 4554.562138671875\n",
            "Epoch 2/50, Loss: 4489.57748046875\n",
            "Epoch 3/50, Loss: 4419.044677734375\n",
            "Epoch 4/50, Loss: 4346.7709375\n",
            "Epoch 5/50, Loss: 4278.60298828125\n",
            "Epoch 6/50, Loss: 4217.185546875\n",
            "Epoch 7/50, Loss: 4161.4303125\n",
            "Epoch 8/50, Loss: 4109.73912109375\n",
            "Epoch 9/50, Loss: 4060.785849609375\n",
            "Epoch 10/50, Loss: 4014.014921875\n",
            "Epoch 11/50, Loss: 3968.745673828125\n",
            "Epoch 12/50, Loss: 3924.446103515625\n",
            "Epoch 13/50, Loss: 3881.252998046875\n",
            "Epoch 14/50, Loss: 3837.881875\n",
            "Epoch 15/50, Loss: 3792.205068359375\n",
            "Epoch 16/50, Loss: 3742.150888671875\n",
            "Epoch 17/50, Loss: 3692.950302734375\n",
            "Epoch 18/50, Loss: 3646.034990234375\n",
            "Epoch 19/50, Loss: 3600.480322265625\n",
            "Epoch 20/50, Loss: 3555.547548828125\n",
            "Epoch 21/50, Loss: 3508.585771484375\n",
            "Epoch 22/50, Loss: 3458.065625\n",
            "Epoch 23/50, Loss: 3407.967119140625\n",
            "Epoch 24/50, Loss: 3360.233095703125\n",
            "Epoch 25/50, Loss: 3314.29654296875\n",
            "Epoch 26/50, Loss: 3269.6578125\n",
            "Epoch 27/50, Loss: 3226.197021484375\n",
            "Epoch 28/50, Loss: 3183.645478515625\n",
            "Epoch 29/50, Loss: 3141.881025390625\n",
            "Epoch 30/50, Loss: 3100.8541796875\n",
            "Epoch 31/50, Loss: 3060.49951171875\n",
            "Epoch 32/50, Loss: 3020.757470703125\n",
            "Epoch 33/50, Loss: 2981.51484375\n",
            "Epoch 34/50, Loss: 2942.384833984375\n",
            "Epoch 35/50, Loss: 2901.114677734375\n",
            "Epoch 36/50, Loss: 2852.18462890625\n",
            "Epoch 37/50, Loss: 2803.420947265625\n",
            "Epoch 38/50, Loss: 2758.425458984375\n",
            "Epoch 39/50, Loss: 2715.5442578125\n",
            "Epoch 40/50, Loss: 2674.199306640625\n",
            "Epoch 41/50, Loss: 2633.990361328125\n",
            "Epoch 42/50, Loss: 2595.042431640625\n",
            "Epoch 43/50, Loss: 2556.646181640625\n",
            "Epoch 44/50, Loss: 2519.0574365234374\n",
            "Epoch 45/50, Loss: 2482.320390625\n",
            "Epoch 46/50, Loss: 2446.0921240234375\n",
            "Epoch 47/50, Loss: 2410.4778759765627\n",
            "Epoch 48/50, Loss: 2375.46181640625\n",
            "Epoch 49/50, Loss: 2341.09208984375\n",
            "Epoch 50/50, Loss: 2307.1272021484374\n",
            "Testing: [32, 16], Sigmoid, Epochs=50, LR=0.001, Batch=128\n",
            "Epoch 1/50, Loss: 4660.835797991072\n",
            "Epoch 2/50, Loss: 4650.372209821428\n",
            "Epoch 3/50, Loss: 4639.354143415178\n",
            "Epoch 4/50, Loss: 4572.477608816965\n",
            "Epoch 5/50, Loss: 4542.033586774553\n",
            "Epoch 6/50, Loss: 4584.373116629465\n",
            "Epoch 7/50, Loss: 4523.64306640625\n",
            "Epoch 8/50, Loss: 4499.243024553572\n",
            "Epoch 9/50, Loss: 4501.550711495535\n",
            "Epoch 10/50, Loss: 4593.926478794643\n",
            "Epoch 11/50, Loss: 4474.39470563616\n",
            "Epoch 12/50, Loss: 4471.884137834822\n",
            "Epoch 13/50, Loss: 4434.478166852678\n",
            "Epoch 14/50, Loss: 4459.459891183035\n",
            "Epoch 15/50, Loss: 4478.2734375\n",
            "Epoch 16/50, Loss: 4391.299211774553\n",
            "Epoch 17/50, Loss: 4401.812918526785\n",
            "Epoch 18/50, Loss: 4376.162039620535\n",
            "Epoch 19/50, Loss: 4357.270089285715\n",
            "Epoch 20/50, Loss: 4323.288713727678\n",
            "Epoch 21/50, Loss: 4368.056849888393\n",
            "Epoch 22/50, Loss: 4313.6943359375\n",
            "Epoch 23/50, Loss: 4369.388323102678\n",
            "Epoch 24/50, Loss: 4325.289899553572\n",
            "Epoch 25/50, Loss: 4294.353864397322\n",
            "Epoch 26/50, Loss: 4246.446428571428\n",
            "Epoch 27/50, Loss: 4241.17902483259\n",
            "Epoch 28/50, Loss: 4247.609514508928\n",
            "Epoch 29/50, Loss: 4205.933663504465\n",
            "Epoch 30/50, Loss: 4231.935407366072\n",
            "Epoch 31/50, Loss: 4208.418073381697\n",
            "Epoch 32/50, Loss: 4213.427525111607\n",
            "Epoch 33/50, Loss: 4182.386579241072\n",
            "Epoch 34/50, Loss: 4243.408342633928\n",
            "Epoch 35/50, Loss: 4143.39045061384\n",
            "Epoch 36/50, Loss: 4080.725516183036\n",
            "Epoch 37/50, Loss: 4066.031912667411\n",
            "Epoch 38/50, Loss: 4055.588692801339\n",
            "Epoch 39/50, Loss: 4098.832624162947\n",
            "Epoch 40/50, Loss: 4094.0836007254466\n",
            "Epoch 41/50, Loss: 4036.2862374441966\n",
            "Epoch 42/50, Loss: 4051.12841796875\n",
            "Epoch 43/50, Loss: 4034.8138602120534\n",
            "Epoch 44/50, Loss: 4024.2030203683034\n",
            "Epoch 45/50, Loss: 3964.9566476004466\n",
            "Epoch 46/50, Loss: 3940.5504324776784\n",
            "Epoch 47/50, Loss: 4009.9979073660716\n",
            "Epoch 48/50, Loss: 3903.493477957589\n",
            "Epoch 49/50, Loss: 3895.9779924665177\n",
            "Epoch 50/50, Loss: 3894.776192801339\n",
            "Testing: [64, 32, 16], ReLU, Epochs=10, LR=0.01, Batch=32\n",
            "Epoch 1/10, Loss: 3090.7362170410156\n",
            "Epoch 2/10, Loss: 262.42981719970703\n",
            "Epoch 3/10, Loss: 93.50821807861328\n",
            "Epoch 4/10, Loss: 69.38147506713867\n",
            "Epoch 5/10, Loss: 57.52374038696289\n",
            "Epoch 6/10, Loss: 49.8582585144043\n",
            "Epoch 7/10, Loss: 43.12064781188965\n",
            "Epoch 8/10, Loss: 37.208541793823244\n",
            "Epoch 9/10, Loss: 33.08328964233399\n",
            "Epoch 10/10, Loss: 31.894494857788086\n",
            "Testing: [64, 32, 16], ReLU, Epochs=10, LR=0.01, Batch=128\n",
            "Epoch 1/10, Loss: 4492.261021205357\n",
            "Epoch 2/10, Loss: 3597.9373604910716\n",
            "Epoch 3/10, Loss: 862.4110565185547\n",
            "Epoch 4/10, Loss: 535.0042332240513\n",
            "Epoch 5/10, Loss: 301.30013166155135\n",
            "Epoch 6/10, Loss: 173.2634495326451\n",
            "Epoch 7/10, Loss: 148.50688934326172\n",
            "Epoch 8/10, Loss: 106.17911202566964\n",
            "Epoch 9/10, Loss: 88.28257424490792\n",
            "Epoch 10/10, Loss: 83.32295880998883\n",
            "Testing: [64, 32, 16], ReLU, Epochs=10, LR=0.001, Batch=32\n",
            "Epoch 1/10, Loss: 4623.937314453125\n",
            "Epoch 2/10, Loss: 4564.580009765625\n",
            "Epoch 3/10, Loss: 4280.6858984375\n",
            "Epoch 4/10, Loss: 3173.368583984375\n",
            "Epoch 5/10, Loss: 1064.544326171875\n",
            "Epoch 6/10, Loss: 169.401015625\n",
            "Epoch 7/10, Loss: 131.73218688964843\n",
            "Epoch 8/10, Loss: 123.21188232421875\n",
            "Epoch 9/10, Loss: 116.52099853515625\n",
            "Epoch 10/10, Loss: 110.76107116699218\n",
            "Testing: [64, 32, 16], ReLU, Epochs=10, LR=0.001, Batch=128\n",
            "Epoch 1/10, Loss: 4600.816545758928\n",
            "Epoch 2/10, Loss: 4560.504813058035\n",
            "Epoch 3/10, Loss: 4657.216378348215\n",
            "Epoch 4/10, Loss: 4696.651436941965\n",
            "Epoch 5/10, Loss: 4598.549386160715\n",
            "Epoch 6/10, Loss: 4534.39453125\n",
            "Epoch 7/10, Loss: 4525.542829241072\n",
            "Epoch 8/10, Loss: 4395.320207868303\n",
            "Epoch 9/10, Loss: 4252.725620814732\n",
            "Epoch 10/10, Loss: 4028.667898995536\n",
            "Testing: [64, 32, 16], ReLU, Epochs=50, LR=0.01, Batch=32\n",
            "Epoch 1/50, Loss: 2716.013250732422\n",
            "Epoch 2/50, Loss: 207.19775756835938\n",
            "Epoch 3/50, Loss: 94.61993896484375\n",
            "Epoch 4/50, Loss: 70.63773971557617\n",
            "Epoch 5/50, Loss: 61.076369781494144\n",
            "Epoch 6/50, Loss: 53.55174705505371\n",
            "Epoch 7/50, Loss: 49.745611877441405\n",
            "Epoch 8/50, Loss: 43.90188446044922\n",
            "Epoch 9/50, Loss: 38.19654319763183\n",
            "Epoch 10/50, Loss: 34.17558692932129\n",
            "Epoch 11/50, Loss: 32.33890167236328\n",
            "Epoch 12/50, Loss: 29.928248748779296\n",
            "Epoch 13/50, Loss: 29.301523056030273\n",
            "Epoch 14/50, Loss: 30.60132698059082\n",
            "Epoch 15/50, Loss: 31.51388069152832\n",
            "Epoch 16/50, Loss: 28.600024185180665\n",
            "Epoch 17/50, Loss: 28.053747329711914\n",
            "Epoch 18/50, Loss: 28.564414291381837\n",
            "Epoch 19/50, Loss: 28.15504852294922\n",
            "Epoch 20/50, Loss: 28.90902229309082\n",
            "Epoch 21/50, Loss: 28.72483024597168\n",
            "Epoch 22/50, Loss: 28.070737762451174\n",
            "Epoch 23/50, Loss: 28.802894363403322\n",
            "Epoch 24/50, Loss: 27.38377628326416\n",
            "Epoch 25/50, Loss: 26.742764129638672\n",
            "Epoch 26/50, Loss: 29.199621353149414\n",
            "Epoch 27/50, Loss: 29.672636451721193\n",
            "Epoch 28/50, Loss: 31.53757987976074\n",
            "Epoch 29/50, Loss: 27.665157623291016\n",
            "Epoch 30/50, Loss: 26.37661071777344\n",
            "Epoch 31/50, Loss: 27.856747970581054\n",
            "Epoch 32/50, Loss: 29.06022804260254\n",
            "Epoch 33/50, Loss: 26.498049926757812\n",
            "Epoch 34/50, Loss: 28.6250439453125\n",
            "Epoch 35/50, Loss: 26.391501808166502\n",
            "Epoch 36/50, Loss: 26.886891555786132\n",
            "Epoch 37/50, Loss: 25.850426712036132\n",
            "Epoch 38/50, Loss: 26.072640151977538\n",
            "Epoch 39/50, Loss: 25.054575729370118\n",
            "Epoch 40/50, Loss: 27.204296073913575\n",
            "Epoch 41/50, Loss: 26.70322189331055\n",
            "Epoch 42/50, Loss: 25.95821907043457\n",
            "Epoch 43/50, Loss: 26.669224014282225\n",
            "Epoch 44/50, Loss: 26.354934272766112\n",
            "Epoch 45/50, Loss: 28.558620376586916\n",
            "Epoch 46/50, Loss: 29.364845962524413\n",
            "Epoch 47/50, Loss: 30.65197311401367\n",
            "Epoch 48/50, Loss: 28.14897476196289\n",
            "Epoch 49/50, Loss: 25.740545654296874\n",
            "Epoch 50/50, Loss: 26.336835861206055\n",
            "Testing: [64, 32, 16], ReLU, Epochs=50, LR=0.01, Batch=128\n",
            "Epoch 1/50, Loss: 4660.634207589285\n",
            "Epoch 2/50, Loss: 4569.292271205357\n",
            "Epoch 3/50, Loss: 4082.938162667411\n",
            "Epoch 4/50, Loss: 2656.57901436942\n",
            "Epoch 5/50, Loss: 480.06105041503906\n",
            "Epoch 6/50, Loss: 524.3486665998187\n",
            "Epoch 7/50, Loss: 225.26068006243025\n",
            "Epoch 8/50, Loss: 211.52694811139787\n",
            "Epoch 9/50, Loss: 135.60374668666296\n",
            "Epoch 10/50, Loss: 111.2090573992048\n",
            "Epoch 11/50, Loss: 104.22005462646484\n",
            "Epoch 12/50, Loss: 92.19312722342355\n",
            "Epoch 13/50, Loss: 89.8701902117048\n",
            "Epoch 14/50, Loss: 84.87178911481585\n",
            "Epoch 15/50, Loss: 82.34473092215401\n",
            "Epoch 16/50, Loss: 80.15317753383091\n",
            "Epoch 17/50, Loss: 78.33077457972935\n",
            "Epoch 18/50, Loss: 75.31646074567523\n",
            "Epoch 19/50, Loss: 73.11918258666992\n",
            "Epoch 20/50, Loss: 69.51154163905552\n",
            "Epoch 21/50, Loss: 67.43234361921039\n",
            "Epoch 22/50, Loss: 64.01948874337333\n",
            "Epoch 23/50, Loss: 62.60794612339565\n",
            "Epoch 24/50, Loss: 61.43966456821987\n",
            "Epoch 25/50, Loss: 56.98468235560826\n",
            "Epoch 26/50, Loss: 56.36644526890346\n",
            "Epoch 27/50, Loss: 64.32309559413365\n",
            "Epoch 28/50, Loss: 57.271823338099885\n",
            "Epoch 29/50, Loss: 51.60479245867048\n",
            "Epoch 30/50, Loss: 53.55110168457031\n",
            "Epoch 31/50, Loss: 48.87080192565918\n",
            "Epoch 32/50, Loss: 47.644906997680664\n",
            "Epoch 33/50, Loss: 47.927759987967356\n",
            "Epoch 34/50, Loss: 49.74639674595424\n",
            "Epoch 35/50, Loss: 46.06390271868025\n",
            "Epoch 36/50, Loss: 44.35487692696707\n",
            "Epoch 37/50, Loss: 43.51146643502371\n",
            "Epoch 38/50, Loss: 41.768880299159456\n",
            "Epoch 39/50, Loss: 42.19312613351004\n",
            "Epoch 40/50, Loss: 42.13173811776297\n",
            "Epoch 41/50, Loss: 41.01084191458566\n",
            "Epoch 42/50, Loss: 40.07323455810547\n",
            "Epoch 43/50, Loss: 40.18936811174665\n",
            "Epoch 44/50, Loss: 38.33853094918387\n",
            "Epoch 45/50, Loss: 37.452493395124165\n",
            "Epoch 46/50, Loss: 34.78387669154576\n",
            "Epoch 47/50, Loss: 35.72810309273856\n",
            "Epoch 48/50, Loss: 34.52636800493513\n",
            "Epoch 49/50, Loss: 33.50507599966867\n",
            "Epoch 50/50, Loss: 33.95917347499302\n",
            "Testing: [64, 32, 16], ReLU, Epochs=50, LR=0.001, Batch=32\n",
            "Epoch 1/50, Loss: 4653.235078125\n",
            "Epoch 2/50, Loss: 4516.638095703125\n",
            "Epoch 3/50, Loss: 3929.20646484375\n",
            "Epoch 4/50, Loss: 2283.9617431640627\n",
            "Epoch 5/50, Loss: 435.48465454101563\n",
            "Epoch 6/50, Loss: 146.84731689453125\n",
            "Epoch 7/50, Loss: 119.19996932983399\n",
            "Epoch 8/50, Loss: 112.0902880859375\n",
            "Epoch 9/50, Loss: 106.04242965698242\n",
            "Epoch 10/50, Loss: 101.07501708984375\n",
            "Epoch 11/50, Loss: 96.26177886962891\n",
            "Epoch 12/50, Loss: 91.94644149780274\n",
            "Epoch 13/50, Loss: 88.35324478149414\n",
            "Epoch 14/50, Loss: 84.6955027770996\n",
            "Epoch 15/50, Loss: 81.1946076965332\n",
            "Epoch 16/50, Loss: 78.25530319213867\n",
            "Epoch 17/50, Loss: 75.32013671875\n",
            "Epoch 18/50, Loss: 73.34599777221679\n",
            "Epoch 19/50, Loss: 70.44265213012696\n",
            "Epoch 20/50, Loss: 68.14502883911133\n",
            "Epoch 21/50, Loss: 66.18245742797852\n",
            "Epoch 22/50, Loss: 64.15692390441895\n",
            "Epoch 23/50, Loss: 62.07816558837891\n",
            "Epoch 24/50, Loss: 60.16783950805664\n",
            "Epoch 25/50, Loss: 58.91892456054688\n",
            "Epoch 26/50, Loss: 57.37825637817383\n",
            "Epoch 27/50, Loss: 55.73634384155273\n",
            "Epoch 28/50, Loss: 54.18510131835937\n",
            "Epoch 29/50, Loss: 52.71900955200195\n",
            "Epoch 30/50, Loss: 51.89094612121582\n",
            "Epoch 31/50, Loss: 50.172996826171875\n",
            "Epoch 32/50, Loss: 49.24129981994629\n",
            "Epoch 33/50, Loss: 48.07761283874512\n",
            "Epoch 34/50, Loss: 47.83890991210937\n",
            "Epoch 35/50, Loss: 46.59182159423828\n",
            "Epoch 36/50, Loss: 44.986451873779295\n",
            "Epoch 37/50, Loss: 44.06490509033203\n",
            "Epoch 38/50, Loss: 43.06837829589844\n",
            "Epoch 39/50, Loss: 42.776788330078126\n",
            "Epoch 40/50, Loss: 41.65756965637207\n",
            "Epoch 41/50, Loss: 40.789968948364255\n",
            "Epoch 42/50, Loss: 40.14814193725586\n",
            "Epoch 43/50, Loss: 39.97297233581543\n",
            "Epoch 44/50, Loss: 38.856709747314454\n",
            "Epoch 45/50, Loss: 38.31792877197265\n",
            "Epoch 46/50, Loss: 37.39447708129883\n",
            "Epoch 47/50, Loss: 36.74669097900391\n",
            "Epoch 48/50, Loss: 36.45962829589844\n",
            "Epoch 49/50, Loss: 35.78819259643555\n",
            "Epoch 50/50, Loss: 35.48018951416016\n",
            "Testing: [64, 32, 16], ReLU, Epochs=50, LR=0.001, Batch=128\n",
            "Epoch 1/50, Loss: 4629.844447544643\n",
            "Epoch 2/50, Loss: 4644.328125\n",
            "Epoch 3/50, Loss: 4623.4921875\n",
            "Epoch 4/50, Loss: 4587.965122767857\n",
            "Epoch 5/50, Loss: 4562.657645089285\n",
            "Epoch 6/50, Loss: 4537.321498325893\n",
            "Epoch 7/50, Loss: 4508.257254464285\n",
            "Epoch 8/50, Loss: 4462.398088727678\n",
            "Epoch 9/50, Loss: 4312.314034598215\n",
            "Epoch 10/50, Loss: 4152.427664620535\n",
            "Epoch 11/50, Loss: 3919.3123953683034\n",
            "Epoch 12/50, Loss: 3530.9192243303573\n",
            "Epoch 13/50, Loss: 3119.2821916852677\n",
            "Epoch 14/50, Loss: 2559.361397879464\n",
            "Epoch 15/50, Loss: 1925.8035365513392\n",
            "Epoch 16/50, Loss: 1280.0687953404017\n",
            "Epoch 17/50, Loss: 699.9747619628906\n",
            "Epoch 18/50, Loss: 308.09430149623324\n",
            "Epoch 19/50, Loss: 163.50832584926061\n",
            "Epoch 20/50, Loss: 147.67503792898995\n",
            "Epoch 21/50, Loss: 159.4914332798549\n",
            "Epoch 22/50, Loss: 142.04845319475447\n",
            "Epoch 23/50, Loss: 135.73929595947266\n",
            "Epoch 24/50, Loss: 129.52652413504464\n",
            "Epoch 25/50, Loss: 123.57327597481864\n",
            "Epoch 26/50, Loss: 119.65093231201172\n",
            "Epoch 27/50, Loss: 119.28835078648159\n",
            "Epoch 28/50, Loss: 119.4839619227818\n",
            "Epoch 29/50, Loss: 120.48547690255302\n",
            "Epoch 30/50, Loss: 116.22907148088727\n",
            "Epoch 31/50, Loss: 117.72534070696149\n",
            "Epoch 32/50, Loss: 114.79223196847099\n",
            "Epoch 33/50, Loss: 111.09130750383649\n",
            "Epoch 34/50, Loss: 112.38804299490792\n",
            "Epoch 35/50, Loss: 104.10812977382115\n",
            "Epoch 36/50, Loss: 105.47506168910435\n",
            "Epoch 37/50, Loss: 103.72816140311105\n",
            "Epoch 38/50, Loss: 105.48729160853794\n",
            "Epoch 39/50, Loss: 107.4673570905413\n",
            "Epoch 40/50, Loss: 103.5047138759068\n",
            "Epoch 41/50, Loss: 98.87850516183036\n",
            "Epoch 42/50, Loss: 97.56290653773716\n",
            "Epoch 43/50, Loss: 100.92451477050781\n",
            "Epoch 44/50, Loss: 97.36624690464565\n",
            "Epoch 45/50, Loss: 94.98623984200614\n",
            "Epoch 46/50, Loss: 99.58648027692523\n",
            "Epoch 47/50, Loss: 96.57751137869698\n",
            "Epoch 48/50, Loss: 91.98041207449776\n",
            "Epoch 49/50, Loss: 92.9814954485212\n",
            "Epoch 50/50, Loss: 93.30224718366351\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=10, LR=0.01, Batch=32\n",
            "Epoch 1/10, Loss: 4447.365400390625\n",
            "Epoch 2/10, Loss: 4061.000126953125\n",
            "Epoch 3/10, Loss: 3624.90244140625\n",
            "Epoch 4/10, Loss: 3215.463291015625\n",
            "Epoch 5/10, Loss: 2813.744677734375\n",
            "Epoch 6/10, Loss: 2406.48671875\n",
            "Epoch 7/10, Loss: 2068.3413427734376\n",
            "Epoch 8/10, Loss: 1741.9210546875\n",
            "Epoch 9/10, Loss: 1423.8639599609376\n",
            "Epoch 10/10, Loss: 1168.5011010742187\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=10, LR=0.01, Batch=128\n",
            "Epoch 1/10, Loss: 4699.8310546875\n",
            "Epoch 2/10, Loss: 4482.584751674107\n",
            "Epoch 3/10, Loss: 4381.150041852678\n",
            "Epoch 4/10, Loss: 4277.310511997768\n",
            "Epoch 5/10, Loss: 4223.331577845982\n",
            "Epoch 6/10, Loss: 4106.362165178572\n",
            "Epoch 7/10, Loss: 4032.8910086495534\n",
            "Epoch 8/10, Loss: 3920.417724609375\n",
            "Epoch 9/10, Loss: 3807.0237165178573\n",
            "Epoch 10/10, Loss: 3658.5201241629466\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=10, LR=0.001, Batch=32\n",
            "Epoch 1/10, Loss: 4657.590302734375\n",
            "Epoch 2/10, Loss: 4587.614169921875\n",
            "Epoch 3/10, Loss: 4514.29228515625\n",
            "Epoch 4/10, Loss: 4447.93591796875\n",
            "Epoch 5/10, Loss: 4392.24689453125\n",
            "Epoch 6/10, Loss: 4342.919736328125\n",
            "Epoch 7/10, Loss: 4297.081259765625\n",
            "Epoch 8/10, Loss: 4253.176650390625\n",
            "Epoch 9/10, Loss: 4210.82900390625\n",
            "Epoch 10/10, Loss: 4169.5441796875\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=10, LR=0.001, Batch=128\n",
            "Epoch 1/10, Loss: 4544.763950892857\n",
            "Epoch 2/10, Loss: 4442.301339285715\n",
            "Epoch 3/10, Loss: 4445.104771205357\n",
            "Epoch 4/10, Loss: 4525.541573660715\n",
            "Epoch 5/10, Loss: 4449.068429129465\n",
            "Epoch 6/10, Loss: 4372.156354631697\n",
            "Epoch 7/10, Loss: 4385.603899274553\n",
            "Epoch 8/10, Loss: 4373.287667410715\n",
            "Epoch 9/10, Loss: 4349.18620954241\n",
            "Epoch 10/10, Loss: 4351.948137555803\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=50, LR=0.01, Batch=32\n",
            "Epoch 1/50, Loss: 4425.79111328125\n",
            "Epoch 2/50, Loss: 3982.11326171875\n",
            "Epoch 3/50, Loss: 3596.930048828125\n",
            "Epoch 4/50, Loss: 3253.61357421875\n",
            "Epoch 5/50, Loss: 2932.215283203125\n",
            "Epoch 6/50, Loss: 2601.3476953125\n",
            "Epoch 7/50, Loss: 2291.6343603515625\n",
            "Epoch 8/50, Loss: 1966.55994140625\n",
            "Epoch 9/50, Loss: 1657.9992578125\n",
            "Epoch 10/50, Loss: 1407.0181298828124\n",
            "Epoch 11/50, Loss: 1202.2504516601562\n",
            "Epoch 12/50, Loss: 1030.9609545898438\n",
            "Epoch 13/50, Loss: 887.1723718261719\n",
            "Epoch 14/50, Loss: 766.7117553710938\n",
            "Epoch 15/50, Loss: 667.4208312988281\n",
            "Epoch 16/50, Loss: 582.4638049316407\n",
            "Epoch 17/50, Loss: 513.551875\n",
            "Epoch 18/50, Loss: 455.7977490234375\n",
            "Epoch 19/50, Loss: 408.5656982421875\n",
            "Epoch 20/50, Loss: 370.2151147460938\n",
            "Epoch 21/50, Loss: 338.8820361328125\n",
            "Epoch 22/50, Loss: 313.89304443359373\n",
            "Epoch 23/50, Loss: 293.58179748535156\n",
            "Epoch 24/50, Loss: 275.4602038574219\n",
            "Epoch 25/50, Loss: 258.2511981201172\n",
            "Epoch 26/50, Loss: 245.99908203125\n",
            "Epoch 27/50, Loss: 238.60097412109374\n",
            "Epoch 28/50, Loss: 233.71922973632812\n",
            "Epoch 29/50, Loss: 230.51630737304689\n",
            "Epoch 30/50, Loss: 228.61969665527343\n",
            "Epoch 31/50, Loss: 227.4223974609375\n",
            "Epoch 32/50, Loss: 226.62458282470703\n",
            "Epoch 33/50, Loss: 226.2846713256836\n",
            "Epoch 34/50, Loss: 225.9003302001953\n",
            "Epoch 35/50, Loss: 225.7462664794922\n",
            "Epoch 36/50, Loss: 225.61065490722658\n",
            "Epoch 37/50, Loss: 225.6231134033203\n",
            "Epoch 38/50, Loss: 225.51140380859374\n",
            "Epoch 39/50, Loss: 225.51590087890625\n",
            "Epoch 40/50, Loss: 225.47211303710938\n",
            "Epoch 41/50, Loss: 225.50372100830077\n",
            "Epoch 42/50, Loss: 225.47619567871095\n",
            "Epoch 43/50, Loss: 225.5182861328125\n",
            "Epoch 44/50, Loss: 225.46650085449218\n",
            "Epoch 45/50, Loss: 225.47032653808594\n",
            "Epoch 46/50, Loss: 225.47864929199218\n",
            "Epoch 47/50, Loss: 225.45953857421875\n",
            "Epoch 48/50, Loss: 225.45470611572264\n",
            "Epoch 49/50, Loss: 225.4266436767578\n",
            "Epoch 50/50, Loss: 219.75411254882812\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=50, LR=0.01, Batch=128\n",
            "Epoch 1/50, Loss: 4602.990513392857\n",
            "Epoch 2/50, Loss: 4455.460100446428\n",
            "Epoch 3/50, Loss: 4338.512625558035\n",
            "Epoch 4/50, Loss: 4284.660609654018\n",
            "Epoch 5/50, Loss: 4176.802873883928\n",
            "Epoch 6/50, Loss: 4061.096505301339\n",
            "Epoch 7/50, Loss: 4046.39501953125\n",
            "Epoch 8/50, Loss: 3951.5303780691966\n",
            "Epoch 9/50, Loss: 3834.610107421875\n",
            "Epoch 10/50, Loss: 3758.7203194754466\n",
            "Epoch 11/50, Loss: 3732.6490304129466\n",
            "Epoch 12/50, Loss: 3618.7465471540177\n",
            "Epoch 13/50, Loss: 3553.993582589286\n",
            "Epoch 14/50, Loss: 3458.6926618303573\n",
            "Epoch 15/50, Loss: 3346.834054129464\n",
            "Epoch 16/50, Loss: 3257.7217843191966\n",
            "Epoch 17/50, Loss: 3165.817940848214\n",
            "Epoch 18/50, Loss: 3047.4321637834823\n",
            "Epoch 19/50, Loss: 2952.1644461495534\n",
            "Epoch 20/50, Loss: 2902.291748046875\n",
            "Epoch 21/50, Loss: 2741.022705078125\n",
            "Epoch 22/50, Loss: 2711.916329520089\n",
            "Epoch 23/50, Loss: 2616.4507882254466\n",
            "Epoch 24/50, Loss: 2554.9073311941966\n",
            "Epoch 25/50, Loss: 2545.6435198102677\n",
            "Epoch 26/50, Loss: 2419.2965262276784\n",
            "Epoch 27/50, Loss: 2332.2352469308034\n",
            "Epoch 28/50, Loss: 2239.8209054129466\n",
            "Epoch 29/50, Loss: 2208.9107840401784\n",
            "Epoch 30/50, Loss: 2065.4468819754466\n",
            "Epoch 31/50, Loss: 2020.0037493024554\n",
            "Epoch 32/50, Loss: 1931.617919921875\n",
            "Epoch 33/50, Loss: 1860.8970772879463\n",
            "Epoch 34/50, Loss: 1744.5856759207588\n",
            "Epoch 35/50, Loss: 1688.3013218470983\n",
            "Epoch 36/50, Loss: 1628.2039794921875\n",
            "Epoch 37/50, Loss: 1560.7561558314733\n",
            "Epoch 38/50, Loss: 1521.1180594308037\n",
            "Epoch 39/50, Loss: 1431.234340122768\n",
            "Epoch 40/50, Loss: 1382.4290248325892\n",
            "Epoch 41/50, Loss: 1354.6224365234375\n",
            "Epoch 42/50, Loss: 1258.254917689732\n",
            "Epoch 43/50, Loss: 1232.9600481305804\n",
            "Epoch 44/50, Loss: 1186.595738002232\n",
            "Epoch 45/50, Loss: 1127.532950265067\n",
            "Epoch 46/50, Loss: 1132.0781773158483\n",
            "Epoch 47/50, Loss: 1065.0445033482142\n",
            "Epoch 48/50, Loss: 1019.6989397321429\n",
            "Epoch 49/50, Loss: 965.9426705496652\n",
            "Epoch 50/50, Loss: 971.1470860072544\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=50, LR=0.001, Batch=32\n",
            "Epoch 1/50, Loss: 4616.21150390625\n",
            "Epoch 2/50, Loss: 4551.63732421875\n",
            "Epoch 3/50, Loss: 4478.495927734375\n",
            "Epoch 4/50, Loss: 4404.468388671875\n",
            "Epoch 5/50, Loss: 4339.054951171875\n",
            "Epoch 6/50, Loss: 4282.037275390625\n",
            "Epoch 7/50, Loss: 4229.7282421875\n",
            "Epoch 8/50, Loss: 4178.5027734375\n",
            "Epoch 9/50, Loss: 4124.367197265625\n",
            "Epoch 10/50, Loss: 4070.886279296875\n",
            "Epoch 11/50, Loss: 4020.079287109375\n",
            "Epoch 12/50, Loss: 3971.4887890625\n",
            "Epoch 13/50, Loss: 3924.233017578125\n",
            "Epoch 14/50, Loss: 3878.2766015625\n",
            "Epoch 15/50, Loss: 3833.18490234375\n",
            "Epoch 16/50, Loss: 3788.94240234375\n",
            "Epoch 17/50, Loss: 3745.59009765625\n",
            "Epoch 18/50, Loss: 3702.79927734375\n",
            "Epoch 19/50, Loss: 3660.620166015625\n",
            "Epoch 20/50, Loss: 3619.0862109375\n",
            "Epoch 21/50, Loss: 3578.032197265625\n",
            "Epoch 22/50, Loss: 3537.56134765625\n",
            "Epoch 23/50, Loss: 3497.574736328125\n",
            "Epoch 24/50, Loss: 3458.068076171875\n",
            "Epoch 25/50, Loss: 3418.922734375\n",
            "Epoch 26/50, Loss: 3380.2893359375\n",
            "Epoch 27/50, Loss: 3342.0920703125\n",
            "Epoch 28/50, Loss: 3304.27103515625\n",
            "Epoch 29/50, Loss: 3266.93392578125\n",
            "Epoch 30/50, Loss: 3229.933603515625\n",
            "Epoch 31/50, Loss: 3193.233662109375\n",
            "Epoch 32/50, Loss: 3157.041220703125\n",
            "Epoch 33/50, Loss: 3121.068154296875\n",
            "Epoch 34/50, Loss: 3085.605830078125\n",
            "Epoch 35/50, Loss: 3050.47515625\n",
            "Epoch 36/50, Loss: 3015.51056640625\n",
            "Epoch 37/50, Loss: 2981.07556640625\n",
            "Epoch 38/50, Loss: 2946.89921875\n",
            "Epoch 39/50, Loss: 2913.115439453125\n",
            "Epoch 40/50, Loss: 2879.6180078125\n",
            "Epoch 41/50, Loss: 2846.453818359375\n",
            "Epoch 42/50, Loss: 2813.6390234375\n",
            "Epoch 43/50, Loss: 2780.902998046875\n",
            "Epoch 44/50, Loss: 2748.831376953125\n",
            "Epoch 45/50, Loss: 2716.8385546875\n",
            "Epoch 46/50, Loss: 2685.288955078125\n",
            "Epoch 47/50, Loss: 2653.82970703125\n",
            "Epoch 48/50, Loss: 2622.88439453125\n",
            "Epoch 49/50, Loss: 2592.16380859375\n",
            "Epoch 50/50, Loss: 2561.67658203125\n",
            "Testing: [64, 32, 16], Sigmoid, Epochs=50, LR=0.001, Batch=128\n",
            "Epoch 1/50, Loss: 4638.961077008928\n",
            "Epoch 2/50, Loss: 4599.236258370535\n",
            "Epoch 3/50, Loss: 4572.77001953125\n",
            "Epoch 4/50, Loss: 4544.894670758928\n",
            "Epoch 5/50, Loss: 4531.3466796875\n",
            "Epoch 6/50, Loss: 4486.683175223215\n",
            "Epoch 7/50, Loss: 4495.798130580357\n",
            "Epoch 8/50, Loss: 4470.976004464285\n",
            "Epoch 9/50, Loss: 4415.364641462053\n",
            "Epoch 10/50, Loss: 4412.651785714285\n",
            "Epoch 11/50, Loss: 4427.393345424107\n",
            "Epoch 12/50, Loss: 4409.322230747768\n",
            "Epoch 13/50, Loss: 4377.769740513393\n",
            "Epoch 14/50, Loss: 4397.864885602678\n",
            "Epoch 15/50, Loss: 4382.400669642857\n",
            "Epoch 16/50, Loss: 4318.694893973215\n",
            "Epoch 17/50, Loss: 4335.162806919643\n",
            "Epoch 18/50, Loss: 4244.44580078125\n",
            "Epoch 19/50, Loss: 4294.642438616072\n",
            "Epoch 20/50, Loss: 4300.763811383928\n",
            "Epoch 21/50, Loss: 4328.159598214285\n",
            "Epoch 22/50, Loss: 4187.07195172991\n",
            "Epoch 23/50, Loss: 4248.895228794643\n",
            "Epoch 24/50, Loss: 4244.191615513393\n",
            "Epoch 25/50, Loss: 4185.476981026785\n",
            "Epoch 26/50, Loss: 4171.112653459822\n",
            "Epoch 27/50, Loss: 4220.521519252232\n",
            "Epoch 28/50, Loss: 4175.714878627232\n",
            "Epoch 29/50, Loss: 4180.591448102678\n",
            "Epoch 30/50, Loss: 4157.031982421875\n",
            "Epoch 31/50, Loss: 4175.510532924107\n",
            "Epoch 32/50, Loss: 4127.579136439732\n",
            "Epoch 33/50, Loss: 4137.900460379465\n",
            "Epoch 34/50, Loss: 4115.322160993303\n",
            "Epoch 35/50, Loss: 4118.222760881697\n",
            "Epoch 36/50, Loss: 4163.297816685268\n",
            "Epoch 37/50, Loss: 4089.8205915178573\n",
            "Epoch 38/50, Loss: 4090.5938546316966\n",
            "Epoch 39/50, Loss: 4059.9820382254466\n",
            "Epoch 40/50, Loss: 4112.580322265625\n",
            "Epoch 41/50, Loss: 3978.669119698661\n",
            "Epoch 42/50, Loss: 3967.7281668526784\n",
            "Epoch 43/50, Loss: 4013.242919921875\n",
            "Epoch 44/50, Loss: 3964.0482003348216\n",
            "Epoch 45/50, Loss: 3947.3593401227677\n",
            "Epoch 46/50, Loss: 3972.6595633370534\n",
            "Epoch 47/50, Loss: 3927.7743791852677\n",
            "Epoch 48/50, Loss: 3977.441336495536\n",
            "Epoch 49/50, Loss: 3942.627755301339\n",
            "Epoch 50/50, Loss: 3944.6932198660716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test_tensor).squeeze()\n",
        "        mse = nn.MSELoss()(y_pred, y_test_tensor)\n",
        "    print(f\"Test MSE: {mse.item()}\")\n",
        "\n",
        "# Evaluate model\n",
        "evaluate_model(model, X_test_tensor, y_test_tensor)\n"
      ],
      "metadata": {
        "id": "FVkvN1ZYbgD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f905c9-d36b-4fe1-9cb2-b2e9c5b8dcdc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 3702.96337890625\n"
          ]
        }
      ]
    }
  ]
}