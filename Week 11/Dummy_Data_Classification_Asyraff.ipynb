{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOphdsrrSF/rBc5dpDMOON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asyraffatha/Task-MachineLearning/blob/main/Week%2011/Dummy_Data_Classification_Asyraff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "krpUdY_SzHqa"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-c1M4czpmG",
        "outputId": "d61c66aa-eb71-4f3f-84ef-aa9fcebce64d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Memuat data\n",
        "file_path = '/content/drive/MyDrive/Week 11/heart.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "FO5fqZs5z2nW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan beberapa baris pertama\n",
        "print(\"Data Overview:\\n\", data.head())\n",
        "print(\"\\nData Info:\\n\")\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-gqm2a90fSF",
        "outputId": "c5af05d9-da40-41df-c500-c3d72bb6b10a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Overview:\n",
            "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n",
            "\n",
            "Data Info:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1025 entries, 0 to 1024\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1025 non-null   int64  \n",
            " 1   sex       1025 non-null   int64  \n",
            " 2   cp        1025 non-null   int64  \n",
            " 3   trestbps  1025 non-null   int64  \n",
            " 4   chol      1025 non-null   int64  \n",
            " 5   fbs       1025 non-null   int64  \n",
            " 6   restecg   1025 non-null   int64  \n",
            " 7   thalach   1025 non-null   int64  \n",
            " 8   exang     1025 non-null   int64  \n",
            " 9   oldpeak   1025 non-null   float64\n",
            " 10  slope     1025 non-null   int64  \n",
            " 11  ca        1025 non-null   int64  \n",
            " 12  thal      1025 non-null   int64  \n",
            " 13  target    1025 non-null   int64  \n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 112.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocessing\n",
        "# Memisahkan fitur dan target\n",
        "X = data.drop(columns=['target'])\n",
        "y = data['target']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "52zvlNTM0idc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membagi data ke dalam set pelatihan dan pengujian\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tr1ypn4H0lKG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "IQKJ9Imj0ozb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Membuat kelas himpunan data kustom\n",
        "class HeartDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Mmembuat dataset\n",
        "dataset_train = HeartDataset(X_train_tensor, y_train_tensor)\n",
        "dataset_test = HeartDataset(X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "id": "umXT4Mi50uud"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Menentukan kelas model MLP\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activation_fn):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        # Tambahkan lapisan tersembunyi\n",
        "        for hidden_units in hidden_layers:\n",
        "            layers.append(nn.Linear(in_features, hidden_units))\n",
        "            layers.append(activation_fn)\n",
        "            in_features = hidden_units\n",
        "\n",
        "        # Tambahkan lapisan output\n",
        "        layers.append(nn.Linear(in_features, 2))  # Binary classification (2 classes)\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "d1n5sJbE0xDE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Tentukan fungsi pelatihan dan evaluasi\n",
        "def train_and_evaluate_model(hidden_layers, activation_fn, epochs, learning_rate, batch_size):\n",
        "    # Define model\n",
        "    model = MLPClassifier(input_size=X_train.shape[1], hidden_layers=hidden_layers, activation_fn=activation_fn)\n",
        "\n",
        "    # Kerugian dan pengoptimal\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Memeuat data\n",
        "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Latih\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for features, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluasi\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for features, _ in test_loader:\n",
        "            preds = model(features)\n",
        "            all_preds.append(preds.argmax(dim=1).numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    accuracy = accuracy_score(y_test, all_preds)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "b6uKuh0Q01u9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Menjalankan eksperimen dengan berbagai konfigurasi\n",
        "activation_functions = {\n",
        "    \"ReLU\": nn.ReLU(),\n",
        "    \"Sigmoid\": nn.Sigmoid(),\n",
        "    \"Tanh\": nn.Tanh(),\n",
        "    \"Softmax\": nn.Softmax(dim=1),\n",
        "    \"Linear\": nn.Identity()\n",
        "}\n"
      ],
      "metadata": {
        "id": "hQJsXCVZ06jV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_configs = [[4], [8, 4], [16, 8, 4]]\n",
        "epochs_list = [1, 10, 25, 50, 100, 250]\n",
        "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
        "\n",
        "results = []"
      ],
      "metadata": {
        "id": "KnIb8apF1MT0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for activation_name, activation_fn in activation_functions.items():\n",
        "    for hidden_layers in hidden_layer_configs:\n",
        "        for epochs in epochs_list:\n",
        "            for lr in learning_rates:\n",
        "                for batch_size in batch_sizes:\n",
        "                    accuracy = train_and_evaluate_model(\n",
        "                        hidden_layers=hidden_layers,\n",
        "                        activation_fn=activation_fn,\n",
        "                        epochs=epochs,\n",
        "                        learning_rate=lr,\n",
        "                        batch_size=batch_size\n",
        "                    )\n",
        "                    results.append({\n",
        "                        \"Activation\": activation_name,\n",
        "                        \"Hidden Layers\": hidden_layers,\n",
        "                        \"Epochs\": epochs,\n",
        "                        \"Learning Rate\": lr,\n",
        "                        \"Batch Size\": batch_size,\n",
        "                        \"Accuracy\": accuracy\n",
        "                    })\n",
        "\n",
        "# Mengonversi hasil ke DataFrame dan tampilkan\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nExperiment Results:\\n\", results_df.head())\n",
        "\n",
        "# Simpan hasil eksperimen\n",
        "results_df.to_csv('/content/drive/MyDrive/Week 11/experiment_results.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWpv3DwD1M67",
        "outputId": "a83c0ba6-3337-4fb6-ac45-03d0a429cbdc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment Results:\n",
            "   Activation Hidden Layers  Epochs  Learning Rate  Batch Size  Accuracy\n",
            "0       ReLU           [4]       1           10.0          16  0.575610\n",
            "1       ReLU           [4]       1           10.0          32  0.434146\n",
            "2       ReLU           [4]       1           10.0          64  0.673171\n",
            "3       ReLU           [4]       1           10.0         128  0.775610\n",
            "4       ReLU           [4]       1           10.0         256  0.770732\n"
          ]
        }
      ]
    }
  ]
}